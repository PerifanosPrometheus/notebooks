{
  
    
        "post0": {
            "title": "Modeling the distribution of neural networks",
            "content": "# collapse import os import matplotlib.pyplot as plt import numpy as np import seaborn as sns from graspy.plot import heatmap, pairplot from scipy.stats import pearsonr, spearmanr from sklearn.datasets import load_digits from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier from graspologic.embed import AdjacencySpectralEmbed from graspologic.match import GraphMatch from src.io import savefig from src.visualization import set_theme FNAME = os.path.basename(__file__)[:-3] set_theme() np.random.seed(8888) def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, print_out=False, **kws) . . Trying to learn distributions on NNs trained on the same task . Example training and predicted labels . Adapted from Sklearn docs . # collapse # The digits dataset digits = load_digits() # The data that we are interested in is made of 8x8 images of digits, let&#39;s # have a look at the first 4 images, stored in the `images` attribute of the # dataset. If we were working from image files, we could load them using # matplotlib.pyplot.imread. Note that each image must have the same size. For these # images, we know which digit they represent: it is given in the &#39;target&#39; of # the dataset. _, axes = plt.subplots(2, 4) images_and_labels = list(zip(digits.images, digits.target)) for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) ax.set_title(&quot;Training: %i&quot; % label, fontsize=&quot;x-small&quot;) # To apply a classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) # Create a classifier: a support vector classifier # classifier = svm.SVC(gamma=0.001) classifier = MLPClassifier() # Split data into train and test subsets X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.5, shuffle=False ) # We learn the digits on the first half of the digits classifier.fit(X_train, y_train) # Now predict the value of the digit on the second half: predicted = classifier.predict(X_test) images_and_predictions = list(zip(digits.images[n_samples // 2 :], predicted)) for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) ax.set_title(&quot;Prediction: %i&quot; % prediction, fontsize=&quot;x-small&quot;) plt.show() . . Training multiple NNs on the same task . # collapse n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) n_replicates = 8 adjs = [] all_biases = [] all_weights = [] for i in range(n_replicates): X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.3, shuffle=True ) mlp = MLPClassifier(hidden_layer_sizes=(50, 50)) mlp.fit(X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) weights_by_layer = mlp.coefs_ all_biases.append(mlp.intercepts_) all_weights.append(mlp.coefs_) print(f&quot;Test accuracy score for NN {i+1}: {acc}&quot;) n_nodes = 0 for weights in weights_by_layer: n_source, n_target = weights.shape n_nodes += n_source n_nodes += n_target adj = np.zeros((n_nodes, n_nodes)) n_nodes_visited = 0 for i, weights in enumerate(weights_by_layer): n_source, n_target = weights.shape adj[ n_nodes_visited : n_nodes_visited + n_source, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target, ] = weights n_nodes_visited += n_source adjs.append(adj) all_biases = [np.concatenate(b) for b in all_biases] all_biases = np.stack(all_biases).T all_weights = [[w.ravel() for w in weights] for weights in all_weights] all_weights = [np.concatenate(w) for w in all_weights] all_weights = np.stack(all_weights).T . . Test accuracy score for NN 1: 0.9685185185185186 Test accuracy score for NN 2: 0.975925925925926 Test accuracy score for NN 3: 0.9611111111111111 Test accuracy score for NN 4: 0.9777777777777777 Test accuracy score for NN 5: 0.9740740740740741 Test accuracy score for NN 6: 0.9833333333333333 Test accuracy score for NN 7: 0.9592592592592593 Test accuracy score for NN 8: 0.9777777777777777 . Plotting the adjacency matrices for each NN . # collapse vmax = max(map(np.max, adjs)) vmin = min(map(np.min, adjs)) fig, axs = plt.subplots(2, 4, figsize=(20, 10)) for i, ax in enumerate(axs.ravel()): heatmap(adjs[i], cbar=False, vmin=vmin, vmax=vmax, ax=ax, title=f&quot;NN {i + 1}&quot;) fig.suptitle(&quot;Adjacency matrices&quot;, fontsize=&quot;large&quot;, fontweight=&quot;bold&quot;) plt.tight_layout() stashfig(&quot;multi-nn-adjs&quot;) . . Trying to model the weights with something simple . Here I just take the mean adjacency matrix, and do a low-rank decomposition. . # collapse adj_bar = np.mean(np.stack(adjs), axis=0) ase = AdjacencySpectralEmbed(n_components=20) X, Y = ase.fit_transform(adj_bar) P_hat = X @ Y.T heatmap(P_hat, title=&quot;Low-rank weights&quot;) stashfig(&quot;p-hat&quot;) . . Then I make a new NN where the weights are set to this low rank matrix, and see how it performs. . NB: what to do with the bias vectors here? Right now I&#39;m just setting to 0. . # collapse mlp = MLPClassifier(hidden_layer_sizes=(50, 50)) mlp.fit( X_train, y_train ) # dummy fit, just to set parameters like shape of input/output n_nodes_visited = 0 for i, weights in enumerate(weights_by_layer): n_source, n_target = weights.shape mlp.coefs_[i] = P_hat[ n_nodes_visited : n_nodes_visited + n_source, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target, ] n_nodes_visited += n_source mlp.intercepts_[i][:] = 0 y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) print(f&quot;Test accuracy score for NN with low-rank weights: {acc}&quot;) . . Test accuracy score for NN with low-rank weights: 0.10185185185185185 . Why I think this doesn&#39;t work, and what we might be able to do about it . There are two big issues as I see it: . Permuation nonidentifiability in the model | Lack of edge-edge dependence structure in our models Below I investigate the first issue, haven&#39;t thought about what to do for the second | . Plotting the learned weights against each other . If each network has $d$ free weight parameters, and there are $T$ of them, I form the $T$ by $d$ matrix of weights per neural network, and then plot each network&#39;s weights against each other. . # collapse def corrplot(x, y, *args, ax=None, fontsize=&quot;xx-small&quot;, **kwargs): if ax is None: ax = plt.gca() pearsons, _ = pearsonr(x, y) spearmans, _ = spearmanr(x, y) text = r&quot;$ rho_p: $&quot; + f&quot;{pearsons:.3f} n&quot; text += r&quot;$ rho_s: $&quot; + f&quot;{spearmans:.3f}&quot; ax.text(1, 1, text, ha=&quot;right&quot;, va=&quot;top&quot;, transform=ax.transAxes, fontsize=fontsize) pg = pairplot( all_weights, alpha=0.1, title=&quot;Weights&quot;, col_names=[f&quot;NN {i+1}&quot; for i in range(all_weights.shape[1])], ) pg.map_offdiag(corrplot) stashfig( &quot;weight-pairplot&quot;, ) . . Can graph matching fix the permutation nonidentifiability? . Given one neural network architecture, one could permute the labels/orders of the hidden units, and the network would be functionally equivalent. This means that when comparing the architectures of two learned neural networks against each other, there is a nonidentifiability problem caused by this arbitrary permutation. Even if we imagine that two neural networks learned the exact same weights, they are unlikely to look similar at a glance because it is unlikely they learned the same weights and the same permutation. Let&#39;s see if graph matching can help resolve this nonidentifiability. . # collapse heatmap_kws = dict(vmin=vmin, vmax=vmax, cbar=False) fig, axs = plt.subplots(1, 3, figsize=(12, 4)) heatmap(adjs[0], ax=axs[0], title=&quot;NN 1 pre-GM&quot;, **heatmap_kws) heatmap(adjs[1], ax=axs[1], title=&quot;NN 2 pre-GM&quot;, **heatmap_kws) heatmap(adjs[0] - adjs[1], ax=axs[2], title=&quot;Difference&quot;, **heatmap_kws) stashfig(&quot;pre-gm-adjs&quot;) seeds = np.concatenate((np.arange(data.shape[1]), np.arange(len(adj) - 10, len(adj)))) gm = GraphMatch(n_init=20, init_method=&quot;barycenter&quot;) gm.fit(adjs[0], adjs[1], seeds_A=seeds, seeds_B=seeds) perm_inds = gm.perm_inds_ adj_1_matched = adjs[1][np.ix_(perm_inds, perm_inds)].copy() fig, axs = plt.subplots(1, 3, figsize=(12, 4)) heatmap(adjs[0], ax=axs[0], title=&quot;NN 1 post-GM&quot;, **heatmap_kws) heatmap(adj_1_matched, ax=axs[1], title=&quot;NN 2 post-GM&quot;, **heatmap_kws) heatmap(adjs[0] - adj_1_matched, ax=axs[2], title=&quot;Difference&quot;, **heatmap_kws) stashfig(&quot;post-gm-adjs&quot;) fig, axs = plt.subplots(1, 2, figsize=(16, 8)) ax = axs[0] sns.scatterplot(adjs[0].ravel(), adjs[1].ravel(), ax=ax, alpha=0.3, linewidth=0, s=15) corrplot(adjs[0].ravel(), adjs[1].ravel(), ax=ax, fontsize=&quot;medium&quot;) ax.set(title=&quot;Weights pre-GM&quot;, xticks=[], yticks=[]) ax = axs[1] sns.scatterplot( adjs[0].ravel(), adj_1_matched.ravel(), ax=ax, alpha=0.3, linewidth=0, s=15 ) corrplot(adjs[0].ravel(), adj_1_matched.ravel(), ax=ax, fontsize=&quot;medium&quot;) ax.set(title=&quot;Weights post-GM&quot;, xticks=[], yticks=[]) stashfig(&quot;pre-post-weights-gm&quot;) . .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/10/05/nn-distributions.html",
            "relUrl": "/pedigo/graspologic/2020/10/05/nn-distributions.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Maggot embedding alignment",
            "content": "# collapse import os import time import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.stats import ortho_group from sklearn.neighbors import NearestNeighbors import graspologic as gl from graspologic.align import OrthogonalProcrustes, SeedlessProcrustes from graspologic.embed import AdjacencySpectralEmbed, select_dimension from graspologic.plot import pairplot from graspologic.utils import augment_diagonal, pass_to_ranks from src.data import load_metagraph from src.graph import MetaGraph from src.io import savefig from src.visualization import adjplot, set_theme print(f&quot;graspologic version: {gl.__version__}&quot;) print(f&quot;seaborn version: {sns.__version__}&quot;) set_theme() palette = dict(zip([&quot;Left&quot;, &quot;Right&quot;, &quot;OP&quot;, &quot;SP&quot;], sns.color_palette(&quot;Set1&quot;))) FNAME = os.path.basename(__file__)[:-3] def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, fmt=&quot;pdf&quot;, **kws) savefig(name, foldername=FNAME, save_on=True, fmt=&quot;png&quot;, dpi=300, **kws) . . graspologic version: 0.1.0.dev20201001120300 seaborn version: 0.10.1 . Load in the data, use only the known pairs . # collapse mg = load_metagraph(&quot;G&quot;) pair_meta = pd.read_csv( &quot;maggot_models/experiments/graph_match/outs/pair_meta.csv&quot;, index_col=0 ) pair_key = &quot;pair_id&quot; pair_meta = pair_meta[pair_meta[f&quot;{pair_key[:-3]}&quot;].isin(pair_meta.index)] pair_meta = pair_meta.sort_values([&quot;hemisphere&quot;, pair_key]) mg = mg.reindex(pair_meta.index.values, use_ids=True) mg = MetaGraph(mg.adj, pair_meta) n_pairs = len(pair_meta) // 2 left_inds = np.arange(n_pairs) right_inds = left_inds.copy() + n_pairs left_mg = MetaGraph(mg.adj[np.ix_(left_inds, left_inds)], mg.meta.iloc[left_inds]) right_mg = MetaGraph(mg.adj[np.ix_(right_inds, right_inds)], mg.meta.iloc[right_inds]) assert (left_mg.meta[pair_key].values == right_mg.meta[pair_key].values).all() print(f&quot;Working with {n_pairs} pairs.&quot;) . . Working with 1173 pairs. . Plot the adjacency matrices, sorted by known pairs . # collapse fig, axs = plt.subplots(1, 2, figsize=(10, 5)) adjplot( left_mg.adj, plot_type=&quot;scattermap&quot;, sizes=(1, 2), ax=axs[0], title=r&quot;Left $ to$ left&quot;, color=palette[&quot;Left&quot;], ) adjplot( right_mg.adj, plot_type=&quot;scattermap&quot;, sizes=(1, 2), ax=axs[1], title=r&quot;Right $ to$ right&quot;, ) stashfig(&quot;left-right-adjs&quot;) . . Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/left-right-adjs.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/left-right-adjs.png . Embed the ipsilateral subgraphs . Computations are happening in n_components dimensions, where n_components = 8 (dashed line in screeplot) | In many plots, I only show the first 4 dimensions just for clarity | . # collapse def plot_latents(left, right, title=&quot;&quot;): plot_data = np.concatenate([left, right], axis=0) labels = np.array([&quot;Left&quot;] * len(left) + [&quot;Right&quot;] * len(right)) pg = pairplot(plot_data[:, :4], labels=labels, title=title) return pg def screeplot(sing_vals, elbow_inds, color=None, ax=None, label=None): if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 4)) plt.plot(range(1, len(sing_vals) + 1), sing_vals, color=color, label=label) plt.scatter( elbow_inds, sing_vals[elbow_inds - 1], marker=&quot;x&quot;, s=50, zorder=10, color=color ) ax.set(ylabel=&quot;Singular value&quot;, xlabel=&quot;Index&quot;) return ax def embed(adj, n_components=40): elbow_inds, elbow_vals = select_dimension( augment_diagonal(pass_to_ranks(adj)), n_elbows=4 ) elbow_inds = np.array(elbow_inds) ase = AdjacencySpectralEmbed(n_components=n_components) out_latent, in_latent = ase.fit_transform(pass_to_ranks(adj)) return out_latent, in_latent, ase.singular_values_, elbow_inds n_components = 8 max_n_components = 40 left_out_latent, left_in_latent, left_sing_vals, left_elbow_inds = embed( left_mg.adj, n_components=max_n_components ) right_out_latent, right_in_latent, right_sing_vals, right_elbow_inds = embed( right_mg.adj, n_components=max_n_components ) # plot the screeplot fig, ax = plt.subplots(1, 1, figsize=(8, 4)) screeplot(left_sing_vals, left_elbow_inds, color=palette[&quot;Left&quot;], ax=ax, label=&quot;Left&quot;) screeplot( right_sing_vals, right_elbow_inds, color=palette[&quot;Right&quot;], ax=ax, label=&quot;Right&quot; ) ax.legend() ax.axvline(n_components, color=&quot;black&quot;, linewidth=1.5, linestyle=&quot;--&quot;) stashfig(&quot;screeplot&quot;) # plot the latent positions before any kind of alignment plot_latents( left_out_latent, right_out_latent, title=&quot;Out latent positions (no alignment)&quot; ) stashfig(&quot;out-latent-no-align&quot;) plot_latents( left_in_latent, right_in_latent, title=&quot;In latent positions (no alignment)&quot; ) stashfig(&quot;in-latent-no-align&quot;) . . /Users/bpedigo/JHU_code/maggot_models/graspologic/graspologic/embed/ase.py:154: UserWarning: Input graph is not fully connected. Results may notbe optimal. You can compute the largest connected component byusing ``graspologic.utils.get_lcc``. /Users/bpedigo/JHU_code/maggot_models/graspologic/graspologic/embed/ase.py:154: UserWarning: Input graph is not fully connected. Results may notbe optimal. You can compute the largest connected component byusing ``graspologic.utils.get_lcc``. Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/screeplot.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/screeplot.png Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-no-align.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-no-align.png Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/in-latent-no-align.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/in-latent-no-align.png . Align the embeddings . Here I align first using Procrustes, and then using Seedless Procrustes initialized at the Procrustes solution. I will refer to this as &quot;oracle initialization&quot;. . &quot;Diff. norm&quot; below refers to $$ | X R - Y |_F$$ where $R$ is learned by one of the Procrustes methods . # collapse def run_alignments(X, Y): op = OrthogonalProcrustes() X_trans_op = op.fit_transform(X, Y) sp = SeedlessProcrustes(init=&quot;custom&quot;, initial_Q=op.Q_) X_trans_sp = sp.fit_transform(X, Y) return X_trans_op, X_trans_sp def calc_diff_norm(X, Y): return np.linalg.norm(X - Y, ord=&quot;fro&quot;) op_left_out_latent, sp_left_out_latent = run_alignments( left_out_latent, right_out_latent ) op_diff_norm = calc_diff_norm(op_left_out_latent, right_out_latent) sp_diff_norm = calc_diff_norm(sp_left_out_latent, right_out_latent) print(f&quot;Procrustes diff. norm using true pairs: {op_diff_norm}&quot;) print(f&quot;Seedless Procrustes diff. norm using true pairs: {sp_diff_norm}&quot;) . . Procrustes diff. norm using true pairs: 10.007906899476886 Seedless Procrustes diff. norm using true pairs: 10.303727955359674 . # collapse plot_latents( op_left_out_latent, right_out_latent, &quot;Out latent positions (Procrustes alignment)&quot; ) stashfig(&quot;out-latent-procrustes&quot;) plot_latents( sp_left_out_latent, right_out_latent, &quot;Out latent positions (Seedless Procrustes alignment, oracle init)&quot;, ) stashfig(&quot;out-latent-seedless-oracle&quot;) . . Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-procrustes.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-procrustes.png Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-seedless-oracle.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-seedless-oracle.png . # collapse op_left_in_latent, sp_left_in_latent = run_alignments(left_in_latent, right_in_latent) . . Looking at nearest neighbors in the aligned embeddings . I am concatenating the in and out embeddings after learning the alignments separately for each | &quot;P by K&quot; (short for precision by K) is the proportion of nodes for which their true pair is within its K nearest neighbors | I plot the cumulative density of the measure described above. So for a point x, y on the plot below, the result can be read as &quot;y of all nodes have their true pair within their x nearest neighbors in the aligned embedded space&quot;. | I perform the experiment above using both orthogonal Procrustes (OP) and Seedless Procrustes (SP) with oracale initialization | . # collapse def calc_nn_ranks(target, query): n_pairs = len(target) nn = NearestNeighbors(n_neighbors=n_pairs, metric=&quot;euclidean&quot;) nn.fit(target) neigh_inds = nn.kneighbors(query, return_distance=False) true_neigh_inds = np.arange(n_pairs) _, ranks = np.where((neigh_inds == true_neigh_inds[:, None])) return ranks def plot_rank_cdf(x, ax=None, color=None, label=None, max_rank=51): if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 4)) hist, _ = np.histogram(x, bins=np.arange(0, max_rank, 1)) hist = hist / n_pairs ax.plot(np.arange(1, max_rank, 1), hist.cumsum(), color=color, label=label) ax.set(ylabel=&quot;Cumulative P by K&quot;, xlabel=&quot;K&quot;) return ax op_left_composite_latent = np.concatenate( (op_left_out_latent, op_left_in_latent), axis=1 ) sp_left_composite_latent = np.concatenate( (sp_left_out_latent, sp_left_in_latent), axis=1 ) right_composite_latent = np.concatenate((right_out_latent, right_in_latent), axis=1) op_ranks = calc_nn_ranks(op_left_composite_latent, right_composite_latent) sp_ranks = calc_nn_ranks(sp_left_composite_latent, right_composite_latent) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) plot_rank_cdf(op_ranks, color=palette[&quot;OP&quot;], ax=ax, label=&quot;OP&quot;) plot_rank_cdf(sp_ranks, color=palette[&quot;SP&quot;], ax=ax, label=&quot;SP&quot;) ax.legend() stashfig(&quot;rank-cdf&quot;) . . Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/rank-cdf.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/rank-cdf.png .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/maggot/2020/10/01/maggot-align.html",
            "relUrl": "/pedigo/graspologic/maggot/2020/10/01/maggot-align.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "gzipping a connectome",
            "content": "import gzip edgelist_loc = &quot;maggot_models/data/processed/2020-09-23/G.edgelist&quot; with open(edgelist_loc, &quot;rb&quot;) as f: raw_edgelist = f.read() raw_kb = len(raw_edgelist) / 1000 gzip_edgelist = gzip.compress(raw_edgelist) gzip_kb = len(gzip_edgelist) / 1000 print(f&quot;Raw edgelist size: {raw_kb:.2f} kb&quot;) print(f&quot;Gzipped edgelist size: {gzip_kb:.2f} kb&quot;) print(f&quot;Compression ratio: {gzip_kb/raw_kb:.2f}&quot;) . Raw edgelist size: 2037.68 kb Gzipped edgelist size: 498.20 kb Compression ratio: 0.24 .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/09/25/gzip-connectome.html",
            "relUrl": "/pedigo/graspologic/2020/09/25/gzip-connectome.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Simple random graph models in the latent space framework",
            "content": "import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import seaborn as sns from graspy.embed import AdjacencySpectralEmbed from graspy.models import DCSBMEstimator, EREstimator, SBMEstimator from graspy.models.sbm import _block_to_full, _get_block_indices from graspy.simulations import er_np, sbm rc_dict = { &quot;axes.spines.right&quot;: False, &quot;axes.spines.top&quot;: False, &quot;axes.edgecolor&quot;: &quot;grey&quot;, } for key, val in rc_dict.items(): mpl.rcParams[key] = val context = sns.plotting_context(context=&quot;talk&quot;, font_scale=1) sns.set_context(context) np.random.seed(8888) . Sample from ER model and get expected probabilities . p = 0.3 n = 200 er_graph = er_np(n, p) er_model = EREstimator(directed=False, loops=True) er_model.fit(er_graph) # hacky way of getting ER P matrix er_model.p_mat_[er_model.p_mat_ != 0] = p . Sample from SBM model and get expected probabilities . B = np.array([[0.5, 0.1], [0.1, 0.3]]) community_sizes = [n // 2, n // 2] sbm_graph, labels = sbm(community_sizes, B, return_labels=True, loops=True) # even more hacky way of getting SBM P matrix sbm_model = SBMEstimator(directed=False, loops=True) sbm_model.fit(sbm_graph, y=labels) sbm_model.block_p_ = B _, _, _block_inv = _get_block_indices(labels) sbm_model.p_mat_ = _block_to_full(B, _block_inv, sbm_graph.shape) . Sample from DCSBM model and get expected probabilities . degree_corrections = np.random.beta(2, 2, size=n) for label in np.unique(labels): mask = labels == label degree_corrections[mask] = np.sort(degree_corrections[mask])[::-1] comm_sum = np.sum(degree_corrections[mask]) degree_corrections[mask] = degree_corrections[mask] / comm_sum dcsbm_graph = sbm(community_sizes, B, dc=degree_corrections, loops=True) # super hacky way of getting DCSBM P matrix dcsbm_model = DCSBMEstimator(directed=False, loops=True) dcsbm_model.fit(dcsbm_graph, y=labels) dcsbm_model.block_p_ = B * (n // 2) ** 2 # block_p_ has a different meaning here degree_corrections = degree_corrections.reshape(-1, 1) dcsbm_model.degree_corrections_ = degree_corrections p_mat = _block_to_full(dcsbm_model.block_p_, _block_inv, dcsbm_graph.shape) p_mat = p_mat * np.outer(degree_corrections[:, 0], degree_corrections[:, -1]) dcsbm_model.p_mat_ = p_mat . graphs = [er_graph, sbm_graph, dcsbm_graph] p_mats = [er_model.p_mat_, sbm_model.p_mat_, dcsbm_model.p_mat_] model_names = [&quot;ER&quot;, &quot;SBM&quot;, &quot;DCSBM&quot;] . Embedding the expected and observed graphs with ASE . ase = AdjacencySpectralEmbed(n_components=2, check_lcc=False) # applying a rotation that just makes the plots look nicer theta = np.radians(-30) c, s = np.cos(theta), np.sin(theta) R = np.array(((c, -s), (s, c))) graph_latents = [] for graph in graphs: graph_latent = ase.fit_transform(graph) graph_latents.append(graph_latent @ R) p_mat_latents = [] for p_mat in p_mats: p_mat_latent = ase.fit_transform(p_mat) p_mat_latents.append(p_mat_latent @ R) . Plotting it all together . n_models = len(model_names) n_cols = 4 scale = 3 fig, axs = plt.subplots(n_models, n_cols, figsize=(scale * n_cols, scale * n_models)) def simple_heatmap(mat, ax): sns.heatmap( mat, ax=ax, xticklabels=False, yticklabels=False, cbar=False, cmap=&quot;RdBu_r&quot;, center=0, square=True, vmin=0, vmax=1, ) def simple_scatter(X, ax, y=None): sns.scatterplot( x=X[:, 0], y=X[:, 1], hue=y, s=15, linewidth=0.25, alpha=0.5, ax=ax, legend=False, ) ax.set( xticks=[], yticks=[], xlabel=&quot;&quot;, ylabel=&quot;&quot;, xlim=(-0.4, 1.4), ylim=(-0.4, 1.4) ) y = None for i, model_name in enumerate(model_names): graph = graphs[i] p_mat = p_mats[i] graph_latent = graph_latents[i] p_mat_latent = p_mat_latents[i] if i &gt; 0: y = labels ax = axs[i, 0] simple_heatmap(p_mat, ax) ax.set_ylabel(model_name) ax = axs[i, 1] simple_heatmap(graph, ax) ax = axs[i, 2] simple_scatter(p_mat_latent, ax, y=y) ax = axs[i, 3] simple_scatter(graph_latent, ax, y=y) axs[0, 0].set_title(r&quot;$ mathbf{P} = mathbf{X X^T}$&quot;) axs[0, 1].set_title(r&quot;$ mathbf{G} sim Bernoulli( mathbf{P})$&quot;) axs[0, 2].set_title(r&quot;$ mathbf{X}$&quot;) axs[0, 3].set_title(r&quot;$ mathbf{ hat{X}}$&quot;) . Text(0.5, 1.0, &#39;$ mathbf{ hat{X}}$&#39;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/09/24/latent_model_tutorial.html",
            "relUrl": "/pedigo/graspologic/2020/09/24/latent_model_tutorial.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
  
    
        ,"post5": {
            "title": "Shuffling Alters QAP Results",
            "content": "%pylab inline import sys sys.path sys.path.insert(0, &#39;/Users/asaadeldin/Downloads/GitHub/scipy&#39;) from scipy.optimize import quadratic_assignment . Populating the interactive namespace from numpy and matplotlib . from graspy.simulations import er_corr rho = 0.9 p = 0.5 n = 20 G1, G2 = er_corr(n, p, rho, directed = False, loops = False) . rep = 5 scores = np.zeros(rep) for i in range(rep): res=quadratic_assignment(G1,G2, options={&#39;maximize&#39;:True, &#39;shuffle_input&#39;: False}) scores[i] =res.fun . scores . array([168., 168., 168., 168., 168.]) . rep = 5 scores = np.zeros(rep) for i in range(rep): res=quadratic_assignment(G1,G2, options={&#39;maximize&#39;:True, &#39;rng&#39;: i}) scores[i] =res.fun . scores . array([174., 162., 164., 168., 164.]) .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/31/shuffle_variance.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/31/shuffle_variance.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Can you make the objective function better via gm, compared to the purported 1-1?",
            "content": "# collapse import sys sys.path sys.path.insert(0, &#39;/Users/asaadeldin/Downloads/GitHub/scipy&#39;) from scipy.optimize import quadratic_assignment . . # collapse %pylab inline import pandas as pd from graspy.utils import pass_to_ranks import seaborn as sns . . Populating the interactive namespace from numpy and matplotlib . Experiment Summary . If $A_i$ is the adjacency matrix at time index $i$, then with $n$ time indices, for $i = [1, n-1]$ do $GM(A_i, A_{i+1})$, where $A_i$ and $A_{i+1}$ are pre-matched based on the known 1-1 correspondance. . For each graph pair, run $GM$ $t = 20$ times, with each $t$ corresponding to a different random permutation on $A_{i+1}$. . Internally in GM, $A_{i+1}$ is shuffled, that is $A_{i+1}&#39; = Q A_{i+1} Q^T,$ where $Q$ is sampled uniformly from the set of $m x m$ permutations matrices, where $m$ is the size of the vertex set. . $GM$ is run from the barycenter ($ gamma = 0$). . Compare the objective function values of the matrices with the known matching ($trace (A_i A_{i+1}^T)$) and the average objective function resulting from $t$ runs of $GM(A_i, A_{i+1})$ . # collapse def load_adj(file): df = pd.read_csv(f&#39;org_sig1_max/{file}.csv&#39;, names = [&#39;from&#39;, &#39;to&#39;, &#39;weight&#39;]) return df . . # collapse times = [1,4,11,17,25,34,45,48,52,55,63,69,70,76,80,83,90,97,103,111,117,129,130,132,139,140,146,153,160,167, 174,181,188,192,195,202,209,216,223,229] . . # collapse from scipy.stats import sem t = 20 ofvs = np.zeros((len(times)-1,3)) # [opt_ofv, gm_ofv] for i in range(len(times)-1): # constructing the adjacency matrices Ael = load_adj(times[i]) Bel = load_adj(times[i+1]) nodes = np.concatenate((Ael[&#39;from&#39;],Ael[&#39;to&#39;],Bel[&#39;from&#39;],Bel[&#39;to&#39;]), axis=0) nodes = list(set(nodes)) n = len(nodes) A = np.zeros((n,n)) B = np.zeros((n,n)) row_list_A = [nodes.index(x) for x in Ael[&#39;from&#39;]] col_list_A = [nodes.index(x) for x in Ael[&#39;to&#39;]] A[row_list_A, col_list_A] = Ael[&#39;weight&#39;] row_list_B = [nodes.index(x) for x in Bel[&#39;from&#39;]] col_list_B = [nodes.index(x) for x in Bel[&#39;to&#39;]] B[row_list_B, col_list_B] = Bel[&#39;weight&#39;] A = pass_to_ranks(A) B = pass_to_ranks(B) gm_ofvs = np.zeros(t) for j in range(t): gmp = {&#39;maximize&#39;:True} res = quadratic_assignment(A,B, options=gmp) gm_ofvs[j] = res.fun gm_ofv = np.mean(gm_ofvs) gm_error = sem(gm_ofvs) opt_ofv = (A*B).sum() ofvs[i,:] = [opt_ofv, gm_ofv, 2*gm_error] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.scatter(np.arange(len(times)-1), ofvs[:,0], label = &#39;opt ofv&#39;) #plt.scatter(np.arange(len(times)), ofvs[:,1], label = &#39;gm ofv&#39;) plt.errorbar(np.arange(len(times)-1),ofvs[:,1], ofvs[:,2],label = &#39;average gm ofv +/- 2 s.e.&#39;,marker=&#39;o&#39;, fmt = &#39; &#39; ,capsize=3, elinewidth=1, markeredgewidth=1,color=&#39;orange&#39;) plt.legend() plt.ylabel(&#39;objective function value&#39;) plt.xlabel(&#39;time stamp (A_x &amp; A_{x+1})&#39;) . . Text(0.5, 0, &#39;time stamp (A_x &amp; A_{x+1})&#39;) . Extremely low variance above (error bars not visible) . # collapse plt.scatter(np.arange(len(times)-1), ofvs[:,1]/ofvs[:,0]) plt.hlines(1,0,40,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;y=1 (above means gm maximes ofv more)&#39;) plt.legend() plt.xlabel(&#39;Time Stamp&#39;) plt.ylabel(&#39;gm_ofv / pre-matched_ofv&#39;) . . Text(0, 0.5, &#39;gm_ofv / pre-matched_ofv&#39;) . # collapse df = pd.DataFrame(ofvs,columns=[&quot;Pre Matched OFV&quot;,&quot;Avergae GM OFV&quot;, &quot;2*s.e. GM OFV&quot;]) print(df) . . Pre Matched OFV Avergae GM OFV 2*s.e. GM OFV 0 77.778503 83.203695 6.520387e-15 1 102.363435 105.635371 6.520387e-15 2 539.611816 586.160090 5.216310e-14 3 184.831288 198.412628 2.608155e-14 4 159.324311 203.971815 0.000000e+00 5 984.399156 1152.762630 1.043262e-13 6 1089.477143 1119.627925 0.000000e+00 7 1259.423017 1290.748354 2.086524e-13 8 1585.077755 1730.198821 0.000000e+00 9 1842.297675 2111.213946 2.086524e-13 10 2144.029010 2320.060471 2.086524e-13 11 2031.803286 2092.043201 2.086524e-13 12 1956.629442 2111.907016 2.086524e-13 13 2449.635493 2580.498796 0.000000e+00 14 2488.814694 2536.434492 2.086524e-13 15 2435.361725 2648.848434 2.086524e-13 16 2450.644416 2909.986251 0.000000e+00 17 2799.810641 3086.113044 4.173048e-13 18 2795.063975 3051.412096 2.086524e-13 19 3093.261915 3183.004837 4.173048e-13 20 3483.603820 3653.248466 0.000000e+00 21 3788.502289 3834.393669 2.086524e-13 22 3837.060440 3977.016536 4.173048e-13 23 3625.742003 3800.326356 0.000000e+00 24 3067.810931 3308.850782 2.086524e-13 25 3440.573652 3634.710350 0.000000e+00 26 4605.778803 4669.727638 4.173048e-13 27 4541.053424 4634.822284 4.173048e-13 28 4305.613442 4587.914977 4.173048e-13 29 4103.909802 4283.742180 4.173048e-13 30 3897.553992 4100.491185 4.173048e-13 31 3744.066025 3871.777437 2.086524e-13 32 3206.210282 3391.022125 0.000000e+00 33 3073.053348 3358.433002 4.173048e-13 34 3177.553500 3395.320322 0.000000e+00 35 3332.128518 3368.928739 2.086524e-13 36 3180.781410 3517.218062 0.000000e+00 37 3385.105045 3510.301189 0.000000e+00 38 3229.233901 3407.950538 2.086524e-13 .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/29/known-correspondance.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/29/known-correspondance.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "this is a test",
            "content": "import numpy as np .",
            "url": "https://neurodata.github.io/notebooks/pedigo/test/2020/08/25/test.html",
            "relUrl": "/pedigo/test/2020/08/25/test.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "GM+SS using JAgt Seedless Procrustes",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 75$ and $ vec{n}=[n_1,n_2,n_3] = [25,25,25]$ . for each $ rho in {0.5,0.6, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$, $GM_{SS}$ &amp; $GM_{J.Agt}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ &amp; $GM_{J.Agt}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=50$, $t=10$ . NOTE: The max number of FW iterations here is set at 20. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Description of $GM_{J.Agt}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . With $2^d$ initializations, where $d$ is dimension, use J.Agt&#39;s seedless procrustes to find the optimal orthogonal alignment matrix, $Q$. . let $Phat = hat{X}_1 Q hat{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . # collapse # load in J.Agt data # ratios_j = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) # ratios_ss_j = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) # scores_j = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) # scores_ss_j = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) # rhos = np.arange(5,10.5,0.5) *0.1 # n_p = len(rhos) # ratios_opt_j = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) # ratios_opt_ss_j = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) # scores_opt_j = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) # scores_opt_ss_j = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) . . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) . . # collapse from scipy.stats import sem import seaborn as sns error = np.asarray([sem(ratios_j[i,:]) for i in range(n_p)]) average = np.asarray([np.mean(ratios_j[i,:] ) for i in range(n_p)]) error_j = np.asarray([sem(ratios_ss_j[i,:]) for i in range(n_p)]) average_j = np.asarray([np.mean(ratios_ss_j[i,:] ) for i in range(n_p)]) . . # collapse error_ss = np.asarray([sem(ratios_ss[i,:]) for i in range(n_p)]) average_ss = np.asarray([np.mean(ratios_ss[i,:] ) for i in range(n_p)]) error2 = np.asarray([sem(ratios[i,:]) for i in range(n_p)]) average2 = np.asarray([np.mean(ratios[i,:] ) for i in range(n_p)]) . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) # plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) # plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.errorbar(rhos,average_j, error_j,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+J.Agt&#39;, color=&#39;green&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;r=50, t=10&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7f8588c091c0&gt; . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from .qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr from .jagt import SeedlessProcrustes from graspy.embed import AdjacencySpectralEmbed def run_sim(r, t, n=150, flip=&#39;median&#39;): def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n def _median_sign_flips(X1, X2): X1_medians = np.median(X1, axis=0) X2_medians = np.median(X2, axis=0) val1 = np.sign(X1_medians).astype(int) X1 = np.multiply(val1.reshape(-1, 1).T, X1) val2 = np.sign(X2_medians).astype(int) X2 = np.multiply(val2.reshape(-1, 1).T, X2) return X1, X2 #rhos = 0.1 * np.arange(11)[5:] m = r rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) ratios_opt = np.zeros((n_p,m)) scores_opt = np.zeros((n_p,m)) ratios_opt_ss = np.zeros((n_p,m)) scores_opt_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None ase = AdjacencySpectralEmbed(n_components=3, algorithm=&#39;truncated&#39;) Xhat1 = ase.fit_transform(A1) Xhat2 = ase.fit_transform(A2) if flip==&#39;median&#39;: xhh1, xhh2 = _median_sign_flips(Xhat1, Xhat2) S = xhh1 @ xhh2.T elif flip==&#39;jagt&#39;: sp = SeedlessProcrustes().fit(Xhat1, Xhat2) xhh1 = Xhat1@sp.Q xhh2 = Xhat2 S = xhh1 @ xhh2.T else: S = None for j in range(t): res = quadratic_assignment_sim(A1, A2, True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, S, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, S, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_opt_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_opt_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.legend() plt.savefig(&#39;figure_matchratio.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/24/jagt-gm.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/24/jagt-gm.html",
            "date": " • Aug 24, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Recreating Youngser's R code figures",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . All values were set to best replicate Youngser&#39;s Figure . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0.5,0.6, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ s.e. . This notebook contains figures for $r=50$, $t=10$ . NOTE: The max number of FW iterations here is set at 20 to best replicate Youngser R results. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 odds = [i for i in range(len(rhos)) if i%2==0] n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = np.asarray([sem(ratios[i,:]) for i in range(n_p)]) average = np.asarray([np.mean(ratios[i,:] ) for i in range(n_p)]) error_ss = np.asarray([sem(ratios_ss[i,:]) for i in range(n_p)]) average_ss = np.asarray([np.mean(ratios_ss[i,:] ) for i in range(n_p)]) . . Ali&#39;s Figure (with Python) . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;r=50, t=10&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f8c33d30&gt; . Youngser&#39;s Figure (with R) . . Eyeballing both onto the same figure . # collapse gm_ss_y = [0.01, 0.03, 0.12, 0.42, 0.77, 1.0] gm_y = [0, 0, 0.03, 0.15, 0.62, 1.0] sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.plot(rhos[odds],gm_ss_y, color=&#39;blue&#39;, marker=&#39;^&#39;) plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.plot(rhos[odds],gm_y, color=&#39;red&#39;, marker=&#39;^&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;triangles = youngser, circle = ali&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5ef18bbe0&gt; . # collapse ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . Call &#39;best case&#39; the instance where $Q$ sampled uniformly from the set of $n x n$ permutations matrices is equal to the identity matrix. . As we see from the figures below, for vanilla GM, the best case seems to consistently perform better, but with GM+SS they appear to be consistently about the same . GM+SS . # collapse diff = scores_opt_ss[9,:] - scores_ss[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt_ss[9,:] - ratios_ss[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . GM . # collapse diff = scores_opt[9,:] - scores[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt[9,:] - ratios[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from .qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def run_sim(r, t): def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = r #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) ratios_opt = np.zeros((n_p,m)) scores_opt = np.zeros((n_p,m)) ratios_opt_ss = np.zeros((n_p,m)) scores_opt_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_opt_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_opt_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.legend() plt.savefig(&#39;figure_matchratio.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/19/recreate-youngser.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/19/recreate-youngser.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Graph matching with spectral similarity (8-18, r=100, t=30, 'max_iter' = 20)",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=100$, $t=30$ . NOTE: The max number of FW iterations here is set at 20 to best replicate Youngser R results. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f6a5fd90&gt; . #collapse ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . Call &#39;best case&#39; the instance where $Q$ sampled uniformly from the set of $n x n$ permutations matrices is equal to the identity matrix . GM+SS . # collapse diff = scores_opt_ss[9,:] - scores_ss[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt_ss[9,:] - ratios_ss[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . GM . #collapse diff = scores_opt[9,:] - scores[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . #collapse diff = ratios_opt[9,:] - ratios[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 100 t = 30 #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;r_100_t_50.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(2).html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(2).html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Graph matching with spectral similarity (8-18, r=100, t=30, 'max_iter' = 30)",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=100$, $t=30$ . NOTE: The max number of FW iterations here is set at 30. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f5ed1430&gt; . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 100 t = 30 #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;r_100_t_50.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(1).html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(1).html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Graph matching with spectral similarity",
            "content": "#collapse from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr from graspy.embed import AdjacencySpectralEmbed . . #collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed import seaborn as sns . . #collapse from qap_sim import quadratic_assignment_sim . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter. . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=50$, $t=20$ . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . #collapse def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 1 t = 10 rhos = 0.1 * np.arange(11) ratios2 = np.zeros((11,m)) scores2 = np.zeros((11,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False . . #collapse n = 150 m = 50 t = 20 rhos = 0.1 * np.arange(11) ratios = np.zeros((11,m)) scores = np.zeros((11,m)) ratios_ss = np.zeros((11,m)) scores_ss = np.zeros((11,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False #np.random.seed(8888) for k, rho in enumerate(rhos): for i in range(m): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): seed = k+m+t res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratios[k,i] = match_ratio(res_opt[&#39;col_ind&#39;], n) scores[k,i] = res_opt[&#39;score&#39;] ratios_ss[k,i] = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) scores_ss[k,i] = res_opt_ss[&#39;score&#39;] #ratios[k] = ratios[k]/m . . #collapse from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(11)] average = [np.mean(ratios[i,:] ) for i in range(11)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(11)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(11)] . . #collapse sns.set_context(&#39;talk&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;GM_GM+SS.png&#39;,fmt=&quot;png&quot;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . . &lt;ipython-input-111-9d9e37bc5d45&gt;:8: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument &#34;fmt&#34; which is no longer supported as of 3.3 and will become an error two minor releases later plt.savefig(&#39;GM_GM+SS.png&#39;,fmt=&#34;png&#34;, dpi=150, facecolor=&#34;w&#34;, bbox_inches=&#34;tight&#34;, pad_inches=0.3) . #collapse diff = ratios_ss[9,:] - ratios[9,:] plt.hist(diff, bins=20) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (GM+SS - GM)&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . #collapse left_adj = np.genfromtxt(&#39;left_adj.csv&#39;, delimiter=&#39;,&#39;) right_adj = np.genfromtxt(&#39;right_adj.csv&#39;, delimiter=&#39;,&#39;) . . #collapse def median_sign_flips(X1, X2): X1_medians = np.median(X1, axis=0) X2_medians = np.median(X2, axis=0) val = np.multiply(X1_medians, X2_medians) t = (val &gt; 0) * 2 - 1 X1 = np.multiply(t.reshape(-1, 1).T, X1) return X1, X2 . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/16/ali-gm-ss.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/16/ali-gm-ss.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Comparing multiple graph samples over time using latent distributions",
            "content": "import time import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from graspy.inference import LatentDistributionTest from graspy.simulations import p_from_latent, sample_edges from graspy.utils import symmetrize from hyppo.discrim import DiscrimOneSample from sklearn.metrics import pairwise_distances np.random.seed(8888) sns.set_context(&quot;talk&quot;) mpl.rcParams[&quot;axes.edgecolor&quot;] = &quot;lightgrey&quot; mpl.rcParams[&quot;axes.spines.right&quot;] = False mpl.rcParams[&quot;axes.spines.top&quot;] = False def hardy_weinberg(theta): &quot;&quot;&quot; Maps a value from [0, 1] to the hardy weinberg curve. &quot;&quot;&quot; hw = [theta ** 2, 2 * theta * (1 - theta), (1 - theta) ** 2] return np.array(hw).T def sample_hw_graph(thetas): latent = hardy_weinberg(thetas) p_mat = p_from_latent(latent, rescale=False, loops=False) graph = sample_edges(p_mat, directed=False, loops=False) return (graph, p_mat, latent) . Parameters of the experiment . n_timepoints = 5 n_verts = 100 n_graphs_per_timepoint = 10 deltas = np.linspace(0, 2, n_timepoints) . Distributions in latent space . Let $HW( theta)$ be the Hardy-Weinberg distribution in $ mathbb{R}^3$. . Latent positions are distributed along this curve: $$X sim HW( theta)$$ With the distribution along the curve following a Beta distribution: $$ theta sim Beta(1, 1 + delta)$$ Let $ delta$ be a proxy for &quot;time&quot; . Below I plot the distributions of $ theta$ for each value of $ delta$, where we will use a different value of $ delta$ for each time point. . fig, ax = plt.subplots(1, 1, figsize=(8, 4)) for delta in deltas: thetas = np.random.beta( 1, 1 + delta, 10000 ) # fake # to make the distributions look cleaner sns.distplot(thetas, label=delta, ax=ax) plt.legend(title=r&quot;$ delta$&quot;, bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;) _ = ax.set(ylabel=&quot;Frequency&quot;, yticks=[], xlabel=r&quot;$ theta$&quot;) . Sample latent positions, and then sample graphs . To generate each graph I sample a set of latent positions from the Hardy-Weinberg curve described above. Each time point will have multiple sets of latent positions sampled i.i.d. from the same distribution in latent space, then a single graph is sampled from each set of latent positions. . graphs = [] latents = [] times = [] for t, delta in enumerate(deltas): for i in range(n_graphs_per_timepoint): thetas = np.random.beta(1, 1 + delta, n_verts) graph, pmat, latent = sample_hw_graph(thetas) graphs.append(graph) times.append(t) latents.append(latent) times = np.array(times) . Plot 2 example sets of sampled latent positions for each time point . Here I just show the first two dimensions of true latent positions. From each of these we sample a graph. . fig, axs = plt.subplots( 2, n_timepoints, figsize=(n_timepoints * 4, 8), sharex=True, sharey=False, # TODO fix sharey and labeling ) for t, delta in enumerate(deltas): for i in range(2): ax = axs[i, t] latent = latents[t * n_graphs_per_timepoint + i] plot_latent = pd.DataFrame(latent) sns.scatterplot(data=plot_latent, x=0, y=1, ax=ax, linewidth=0, alpha=0.5, s=20) ax.set(xlabel=&quot;&quot;, ylabel=&quot;&quot;, xticks=[], yticks=[]) if i == 0: deltastr = r&quot;$ delta$&quot; + f&quot; = {deltas[t]}&quot; ax.set_title(f&quot;t = {t} ({deltastr})&quot;) if t == 0: ax.set_ylabel(f&quot;Sample {i + 1}&quot;) plt.tight_layout() . Plot adjacency matrices for 2 graphs from each time point . fig, axs = plt.subplots(2, n_timepoints, figsize=(n_timepoints * 4, 8)) for t, delta in enumerate(deltas): for i in range(2): graph = graphs[t * n_graphs_per_timepoint + i] ax = axs[i, t] sns.heatmap( graph, ax=ax, cbar=False, xticklabels=False, yticklabels=False, cmap=&quot;RdBu_r&quot;, square=True, center=0, ) if i == 0: deltastr = r&quot;$ delta$&quot; + f&quot; = {deltas[t]}&quot; ax.set_title(f&quot;t = {t} ({deltastr})&quot;) if t == 0: ax.set_ylabel(f&quot;Sample {i + 1}&quot;) plt.tight_layout() . Compute the test statistics for Latent Distribution Test (nonpar). . curr_time = time.time() pval_mat = np.zeros((len(graphs), len(graphs))) tstat_mat = np.zeros((len(graphs), len(graphs))) n_comparisons = (len(graphs) * (len(graphs) - 1)) / 2 counter = 0 for i, graph1 in enumerate(graphs): for j, graph2 in enumerate(graphs): if i &lt; j: ldt = LatentDistributionTest(n_bootstraps=200, workers=1) ldt.fit(graph1, graph2) pval_mat[i, j] = ldt.p_value_ tstat_mat[i, j] = ldt.sample_T_statistic_ pval_mat = symmetrize( pval_mat, method=&quot;triu&quot; ) # need to do way more bootstraps to be meaningful tstat_mat = symmetrize(tstat_mat, method=&quot;triu&quot;) print(f&quot;{(time.time() - curr_time)/60:.3f} minutes elapsed&quot;) . 5.981 minutes elapsed . All pairwise test statistics and p-values . Here I show test statistics for the latent position test between all possible pairs of graphs. Higher means more different. The test statistic being used here is the 2-sample dcorr test statistic on the estimated latent positions. Note that I&#39;m not doing the new seedless alignment here (but I&#39;d like to). . Then, I show the same for the p-values. . fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.heatmap( tstat_mat, ax=ax, xticklabels=False, yticklabels=False, cmap=&quot;Reds&quot;, square=True, cbar_kws=dict(shrink=0.7), ) line_kws = dict(linestyle=&quot;-&quot;, linewidth=1, color=&quot;grey&quot;) for t in range(1, n_timepoints): ax.axvline(t * n_graphs_per_timepoint, **line_kws) ax.axhline(t * n_graphs_per_timepoint, **line_kws) tick_locs = ( np.arange(0, n_timepoints * n_graphs_per_timepoint, n_graphs_per_timepoint) + n_graphs_per_timepoint / 2 ) ax.set( xticks=tick_locs, xticklabels=np.arange(n_timepoints), xlabel=&quot;Time point&quot;, title=&quot;Latent distribution test statistics&quot;, ) fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.heatmap( pval_mat, ax=ax, xticklabels=False, yticklabels=False, cmap=&quot;Reds&quot;, square=True, cbar_kws=dict(shrink=0.7), ) line_kws = dict(linestyle=&quot;-&quot;, linewidth=1, color=&quot;grey&quot;) for t in range(1, n_timepoints): ax.axvline(t * n_graphs_per_timepoint, **line_kws) ax.axhline(t * n_graphs_per_timepoint, **line_kws) tick_locs = ( np.arange(0, n_timepoints * n_graphs_per_timepoint, n_graphs_per_timepoint) + n_graphs_per_timepoint / 2 ) _ = ax.set( xticks=tick_locs, xticklabels=np.arange(n_timepoints), xlabel=&quot;Time point&quot;, title=&quot;Latent distribution test p-values&quot;, ) . Computing discriminability . Looks at whether distances between samples from the same object (time point, in this case) are smaller than distances between samples from different objects. In a sense, it&#39;s looking at whether the diagonal blocks in the above are smaller than the rest of the matrix. Here I&#39;m using the test statistic from above as the distance. Permutation test is used to test whether one&#39;s ability to discriminate between &quot;multiple samples&quot; from the same object is highter than one would expect by chance. . curr_time = time.time() discrim = DiscrimOneSample(is_dist=True) discrim.test(tstat_mat, times) print(f&quot;Discriminability one-sample p-value: {discrim.pvalue_}&quot;) print(f&quot;Discriminability test statistic: {discrim.stat}&quot;) print(f&quot;{(time.time() - curr_time)/60:.3f} minutes elapsed&quot;) . Discriminability one-sample p-value: 0.001 Discriminability test statistic: 0.8648333333333332 0.061 minutes elapsed . Test statistics and p-values as a function of time difference . Here I just play with plotting these test statistics and p-values as a function of how different in time the two graphs were. I add jitter to the time difference values just for visibility. . time_dist_mat = pairwise_distances(times.reshape((-1, 1)), metric=&quot;manhattan&quot;) triu_inds = np.triu_indices_from(time_dist_mat, k=1) time_dists = time_dist_mat[triu_inds] + np.random.uniform(-0.2, 0.2, len(triu_inds[0])) latent_dists = tstat_mat[triu_inds] fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.scatterplot(x=time_dists, y=latent_dists, s=10, linewidth=0, alpha=0.3, ax=ax) ax.set(ylabel=&quot;Test statistic&quot;, xlabel=&quot;Difference in time&quot;) pval_dists = pval_mat[triu_inds] fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.scatterplot(x=time_dists, y=pval_dists, s=10, linewidth=0, alpha=0.3, ax=ax) _ = ax.set(ylabel=&quot;p-value&quot;, xlabel=&quot;Difference in time&quot;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/07/23/multi-time-latents.html",
            "relUrl": "/pedigo/graspologic/2020/07/23/multi-time-latents.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://neurodata.github.io/notebooks/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://neurodata.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://neurodata.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://neurodata.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}