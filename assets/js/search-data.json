{
  
    
        "post0": {
            "title": "Joint embedding of connectivity and morphology",
            "content": "# collapse import logging import os import time import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymaid import seaborn as sns from hyppo.discrim import DiscrimOneSample from sklearn.metrics import pairwise_distances from sklearn.preprocessing import QuantileTransformer from umap import UMAP from graspologic.embed import ( AdjacencySpectralEmbed, LaplacianSpectralEmbed, select_dimension, selectSVD, ) from graspologic.plot import pairplot from graspologic.utils import pass_to_ranks, symmetrize from navis import NeuronList, TreeNeuron, nblast_allbyall from src.data import load_metagraph from src.io import savefig from src.pymaid import start_instance from src.visualization import CLASS_COLOR_DICT, adjplot, set_theme # REF: https://stackoverflow.com/questions/35326814/change-level-logged-to-ipython-jupyter-notebook logger = logging.getLogger() assert len(logger.handlers) == 1 handler = logger.handlers[0] handler.setLevel(logging.ERROR) t0 = time.time() # some plotting settings set_theme() mpl.rcParams[&quot;axes.labelcolor&quot;] = &quot;black&quot; mpl.rcParams[&quot;text.color&quot;] = &quot;black&quot; mpl.rcParams[&quot;ytick.color&quot;] = &quot;black&quot; mpl.rcParams[&quot;xtick.color&quot;] = &quot;black&quot; CLASS_COLOR_DICT[&quot;sens&quot;] = CLASS_COLOR_DICT[&quot;sens-ORN&quot;] CLASS_COLOR_DICT[&quot;bLN&quot;] = CLASS_COLOR_DICT[&quot;pLN&quot;] palette = CLASS_COLOR_DICT FNAME = os.path.basename(__file__)[:-3] print(FNAME) def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, print_out=False, **kws) # for pymaid to pull neurons start_instance() . . 204.0-BDP-joint-topology-morphology_python . CatmaidInstance at 140361106948240. Server: https://neurocean.janelia.org/catmaidL1 Project: 1 Caching True (size limit 128; time limit None) Cache size: 0.0 . The data . Here I select a subset of the Drosophila larva connectome. Specifically: . Kenyon cells (KC) | Mushroom body input neurons (MBIN) | Mushroom body output neurons (MBON) | Uniglomerular projection neurons (uPN) | Thermosensitive projection neurons (tPN) | Visual projection neurons (vPN) | Broad and picky local neurons (bLN &amp; pLN) | APL | Larval optic neuropil (LON) | TODO: visual sensory neurons | . For the purposes of the network, I use the &quot;summed&quot; or union graph (not separating by axon/dendrite). I also restrict this analysis to one hemisphere (the left). . # collapse mg = load_metagraph(&quot;G&quot;) meta = mg.meta meta = meta[meta[&quot;left&quot;]] class1 = [&quot;KC&quot;, &quot;MBIN&quot;, &quot;MBON&quot;, &quot;uPN&quot;, &quot;tPN&quot;, &quot;vPN&quot;, &quot;bLN&quot;, &quot;pLN&quot;, &quot;APL&quot;, &quot;LON&quot;] class2 = [&quot;ORN&quot;, &quot;Rh5&quot;, &quot;Rh6&quot;] # TODO fix and make the visual ones work? meta = meta[meta[&quot;class1&quot;].isin(class1) | meta[&quot;class2&quot;].isin(class2)] meta[&quot;merge_class&quot;].unique() mg = mg.reindex(meta.index, use_ids=True) mg = mg.remove_pdiff() mg = mg.make_lcc() meta = mg.meta print(f&quot;{len(meta)} neurons in selected largest connected component&quot;) . . 184 neurons in selected largest connected component . # collapse neuron_ids = [int(n) for n in meta.index] neurons = pymaid.get_neuron(neuron_ids) # load in with pymaid # HACK: I am guessing there is a better way to do the below? # TODO: I was also getting some errors about neurons with more that one soma, so I threw # them out for now. treenode_tables = [] for neuron_id, neuron in zip(neuron_ids, neurons): treenode_table = pymaid.get_treenode_table(neuron, include_details=False) treenode_tables.append(treenode_table) success_neurons = [] tree_neurons = [] for neuron_id, treenode_table in zip(neuron_ids, treenode_tables): treenode_table.rename(columns={&quot;parent_node_id&quot;: &quot;parent_id&quot;}, inplace=True) tree_neuron = TreeNeuron(treenode_table) if (tree_neuron.soma is not None) and (len(tree_neuron.soma) &gt; 1): print(f&quot;Neuron {neuron_id} has more than one soma, removing&quot;) else: tree_neurons.append(tree_neuron) success_neurons.append(neuron_id) tree_neurons = NeuronList(tree_neurons) meta = meta.loc[success_neurons] mg = mg.reindex(success_neurons, use_ids=True) print(f&quot;{len(meta)} neurons ready for NBLAST&quot;) . . Neuron 15571194 has more than one soma, removing Neuron 16977881 has more than one soma, removing 182 neurons ready for NBLAST . # collapse adj = mg.adj ptr_adj = pass_to_ranks(adj) . . Running NBLAST and post-processing the scores . # collapse currtime = time.time() # NOTE: I&#39;ve had too modify original code to allow smat=None # NOTE: this only works when normalized=False also scores = nblast_allbyall(tree_neurons, smat=None, normalized=False, progress=False) print(f&quot;{time.time() - currtime:.3f} elapsed to run NBLAST.&quot;) . . 64.477 elapsed to run NBLAST. . # collapse distance = scores.values # the raw nblast scores are dissimilarities/distances sym_distance = symmetrize(distance) # the raw scores are not symmetric # make the distances between 0 and 1 sym_distance /= sym_distance.max() sym_distance -= sym_distance.min() # and then convert to similarity morph_sim = 1 - sym_distance # rank transform the similarities # NOTE this is very different from what native NBLAST does and could likely be improved # upon a lot. I did this becuase it seemed like a quick way of accounting for difference # in scale for different neurons as well as the fact that the raw distribution of # similaritys was skewed high (very few small values) quant = QuantileTransformer() indices = np.triu_indices_from(morph_sim, k=1) transformed_vals = quant.fit_transform(morph_sim[indices].reshape(-1, 1)) transformed_vals = np.squeeze(transformed_vals) # this is a discrete version of PTR basically ptr_morph_sim = np.ones_like(morph_sim) ptr_morph_sim[indices] = transformed_vals ptr_morph_sim[indices[::-1]] = transformed_vals # # before # plt.figure() # sns.distplot(morph_sim[np.triu_indices_from(morph_sim, k=1)]) # # after # plt.figure() # sns.distplot(ptr_morph_sim[np.triu_indices_from(morph_sim, k=1)]) . . Plotting both modalities . Left: the adjacency matrix for this subgraph after pass-to-ranks. . Right: the similarity matrix obtained from NBLAST after some post-processing, including a quantile transform (like pass-to-ranks). See the code in the cell above for more explanation. Also likely subject to change. . # collapse sns.set_context(&quot;talk&quot;, font_scale=1.25) fig, axs = plt.subplots(1, 2, figsize=(20, 10)) ax = axs[0] adjplot( ptr_adj, sort_class=meta[&quot;class1&quot;], colors=meta[&quot;class1&quot;], palette=palette, tick_rot=45, cbar=False, ax=ax, title=&quot;Connectivity (adjacency matrix)&quot;, ) ax = axs[1] adjplot( ptr_morph_sim, sort_class=meta[&quot;class1&quot;], colors=meta[&quot;class1&quot;], palette=palette, tick_rot=45, cbar=False, ax=ax, title=&quot;Morphology (pairwise NBLAST)&quot;, ) stashfig(&quot;adj-morpho-heatmaps&quot;) . . The goal and the method . We aim to learn a single embedding which is representative of differences in both neuron connectivity and morphology. Ultimately, we wish to cluster (or otherwise look at which neurons are similar/different) using that combined representation. . To do so we will use a MASE-like approach. It consists of two simple stages: . Embed the adjacency matrix (or Lapliacian) and embed the NBLAST similarity matrix | Concatenate the two embeddings from step 1, and then embed that matrix for our final representation. | Since the connectivity embedding is directed, for the concatenation step in 2., I use $$W = left [ U_A, V_A, U_M right ]$$ where $U_A$ and $V_A$ are the left and right singular vectors of the adjacency or Laplacian matrix, and $U_M$ is the singular vectors of the morphology similarity matrix, and $W$ is the matrix which is going to be embedded again for the final representation. . # collapse ptr_adj = pass_to_ranks(adj) def careys_rule(X): &quot;&quot;&quot;Get the number of singular values to check&quot;&quot;&quot; return int(np.ceil(np.log2(np.min(X.shape)))) zg_n_components = careys_rule(adj) max_n_components = 40 # basically just how many to show for screeplots embed_kws = dict(concat=False) embedding = &quot;ASE&quot; if embedding == &quot;ASE&quot;: Embedder = AdjacencySpectralEmbed elif embedding == &quot;LSE&quot;: Embedder = LaplacianSpectralEmbed embed_kws[&quot;form&quot;] = &quot;R-DAD&quot; adj_model = Embedder(n_components=max_n_components, **embed_kws) adj_embed = adj_model.fit_transform(ptr_adj) if embedding == &quot;ASE&quot;: embed_kws[&quot;diag_aug&quot;] = False morph_model = Embedder(n_components=max_n_components, **embed_kws) morph_embed = morph_model.fit_transform(ptr_morph_sim) . . Screeplots for the first stage . # collapse def textplot(x, y, text, ax=None, x_pad=0, y_pad=0, **kwargs): &quot;&quot;&quot;Plot a iterables of x, y, text with matplotlib&#39;s ax.text&quot;&quot;&quot; if ax is None: ax = plt.gca() for x_loc, y_loc, s in zip(x, y, text): ax.text( x_loc + x_pad, y_loc + y_pad, s, transform=ax.transData, **kwargs, ) def screeplot( singular_values, check_n_components=None, ax=None, title=&quot;Screeplot&quot;, n_elbows=4 ): if ax is None: ax = plt.gca() elbows, elbow_vals = select_dimension( singular_values[:check_n_components], n_elbows=n_elbows ) index = np.arange(1, len(singular_values) + 1) sns.lineplot(x=index, y=singular_values, ax=ax, zorder=1) sns.scatterplot( x=elbows, y=elbow_vals, color=&quot;darkred&quot;, marker=&quot;x&quot;, ax=ax, zorder=2, s=80, linewidth=2, ) textplot( elbows, elbow_vals, elbows, ax=ax, color=&quot;darkred&quot;, fontsize=&quot;small&quot;, x_pad=0.5, y_pad=0.5, zorder=3, ) ax.set(title=title, xlabel=&quot;Index&quot;, ylabel=&quot;Singular value&quot;) ax.yaxis.set_major_locator(plt.MaxNLocator(3)) return ax sns.set_context(font_scale=1) fig, axs = plt.subplots(2, 1, figsize=(8, 8)) ax = axs[0] screeplot( adj_model.singular_values_, check_n_components=zg_n_components, ax=ax, title=&quot;Connectivity&quot;, ) ax = axs[1] screeplot( morph_model.singular_values_, check_n_components=zg_n_components, ax=ax, title=&quot;Morphology&quot;, ) plt.tight_layout() stashfig(&quot;stage1-scree&quot;) . . Based on the screeplots above, I selected 5 as the embedding dimension to use for stage 2. . Plot the connectivity and morphology embeddings from the first stage . Here I show the first 6 dimensions of the embeddings (or the first 3 + 3 for a directed embedding). . # collapse n_show = 6 labels = meta[&quot;class1&quot;].values n_show_per = n_show // 2 plot_adj_embed = np.concatenate( (adj_embed[0][:, :n_show_per], adj_embed[1][:, :n_show_per]), axis=1 ) col_names = [f&quot;Out dimension {i + 1}&quot; for i in range(n_show_per)] col_names += [f&quot;In dimension {i + 1}&quot; for i in range(n_show_per)] labels = meta[&quot;class1&quot;].values pairplot( plot_adj_embed, labels=labels, col_names=col_names, palette=palette, title=&quot;Connectivity screeplot&quot;, diag_kind=&quot;hist&quot;, ) stashfig(&quot;connectivity-pairplot&quot;) pairplot( morph_embed[:, :n_show], labels=labels, palette=palette, title=&quot;Morphology screeplot&quot;, diag_kind=&quot;hist&quot;, ) stashfig(&quot;morphology-pairplot&quot;) . . # collapse def unscale(X): # TODO implement as a setting in graspologic norms = np.linalg.norm(X, axis=0) X = X / norms[None, :] return X adj_embed = [unscale(a) for a in adj_embed] unscale(morph_embed) n_components = 5 # setting based on the plots above concat_embed = np.concatenate( ( adj_embed[0][:, :n_components], adj_embed[1][:, :n_components], morph_embed[:, :n_components], ), axis=1, ) joint_embed, joint_singular_values, _ = selectSVD( concat_embed, n_components=concat_embed.shape[1], algorithm=&quot;full&quot; ) . . Screeplot for the second stage . # collapse fig, ax = plt.subplots(1, 1, figsize=(8, 4)) screeplot( joint_singular_values, check_n_components=careys_rule(concat_embed), ax=ax, title=&quot;Joint embedding screeplot&quot;, ) stashfig(&quot;stage2-scree&quot;) . . Plot the joint embedding from stage 2 . # collapse pairplot( joint_embed[:, :n_show], labels=labels, palette=palette, title=&quot;Joint embedding&quot;, diag_kind=&quot;hist&quot;, ) stashfig(&quot;joint-pairplot&quot;) . . # collapse n_components = 10 umap = UMAP(min_dist=0.8, n_neighbors=20, metric=&quot;cosine&quot;) umap_embed = umap.fit_transform(joint_embed[:, :n_components]) def simple_scatterplot(X, labels=labels, palette=&quot;deep&quot;, ax=None, title=&quot;&quot;): if ax is None: _, ax = plt.subplots(1, 1, figsize=(10, 10)) ax.spines[&quot;left&quot;].set_visible(False) ax.spines[&quot;bottom&quot;].set_visible(False) plot_df = pd.DataFrame(data=X[:, :2], columns=[&quot;0&quot;, &quot;1&quot;]) plot_df[&quot;labels&quot;] = labels sns.scatterplot(data=plot_df, x=&quot;0&quot;, y=&quot;1&quot;, hue=&quot;labels&quot;, palette=palette, ax=ax) ax.set(xlabel=&quot;&quot;, ylabel=&quot;&quot;, title=title, xticks=[], yticks=[]) ax.get_legend().remove() return fig, ax simple_scatterplot( umap_embed, labels=labels, palette=palette, title=r&quot;UMAP $ circ$ joint embedding&quot; ) stashfig(&quot;joint-umap&quot;) . . Running discriminability . # collapse uni_labels = np.unique(labels) label_map = dict(zip(uni_labels, range(len(uni_labels)))) int_labels = np.vectorize(label_map.get)(labels) stage1_n_components_range = [3, 5, 7, 10, 15] stage2_n_components_range = [3, 5, 7, 10, 15] metrics = [&quot;euclidean&quot;, &quot;cosine&quot;] rows = [] for stage1_n_components in stage1_n_components_range: concat_embed = np.concatenate( ( adj_embed[0][:, :stage1_n_components], adj_embed[1][:, :stage1_n_components], morph_embed[:, :stage1_n_components], ), axis=1, ) for stage2_n_components in stage2_n_components_range: if stage1_n_components * 3 &lt; stage2_n_components: for metric in metrics: output = { &quot;metric&quot;: metric, &quot;stage2_n_components&quot;: stage2_n_components, &quot;stage1_n_components&quot;: stage1_n_components, &quot;discrim_tstat&quot;: None, &quot;embedding&quot;: &quot;joint&quot;, } rows.append(output) else: joint_embed, joint_singular_values, _ = selectSVD( concat_embed, n_components=stage2_n_components, algorithm=&quot;full&quot; ) X = joint_embed[:, :stage2_n_components] for metric in metrics: pdist = pairwise_distances(X, metric=metric) discrim = DiscrimOneSample(is_dist=True) tstat, _ = discrim.test(pdist, int_labels, reps=0) output = { &quot;metric&quot;: metric, &quot;stage2_n_components&quot;: stage2_n_components, &quot;stage1_n_components&quot;: stage1_n_components, &quot;discrim_tstat&quot;: tstat, &quot;embedding&quot;: &quot;joint&quot;, } rows.append(output) for stage1_n_components in stage1_n_components_range: for embedding in [&quot;connectivity&quot;, &quot;morphology&quot;]: if embedding == &quot;connectivity&quot;: X = np.concatenate( ( adj_embed[0][:, :stage1_n_components], adj_embed[1][:, :stage1_n_components], ), axis=1, ) else: X = morph_embed[:, :stage1_n_components] for metric in metrics: pdist = pairwise_distances(X, metric=metric) discrim = DiscrimOneSample(is_dist=True) tstat, _ = discrim.test(pdist, int_labels, reps=0) output = { &quot;metric&quot;: metric, &quot;stage2_n_components&quot;: None, &quot;stage1_n_components&quot;: stage1_n_components, &quot;discrim_tstat&quot;: tstat, &quot;embedding&quot;: embedding, } rows.append(output) results = pd.DataFrame(rows) . . Discriminability plotted as a function of stage 1 and stage 2 dimension . # collapse pivot_kws = dict( index=&quot;stage1_n_components&quot;, columns=&quot;stage2_n_components&quot;, values=&quot;discrim_tstat&quot; ) heatmap_kws = dict( square=True, annot=True, cbar=False, vmin=results[&quot;discrim_tstat&quot;].min(), vmax=1, cmap=&quot;RdBu_r&quot;, center=results[&quot;discrim_tstat&quot;].min(), ) fig, axs = plt.subplots( 2, 3, figsize=(12, 12), gridspec_kw=dict(width_ratios=[0.3, 0.3, 1]), ) # euclidean, connectivity ax = axs[0, 0] plot_results = results[ (results[&quot;embedding&quot;] == &quot;connectivity&quot;) &amp; (results[&quot;metric&quot;] == &quot;euclidean&quot;) ].pivot(**pivot_kws) sns.heatmap(plot_results.values, ax=ax, **heatmap_kws) ax.set( ylabel=&quot;Metric = Euclidean n nStage 1 # components&quot;, yticklabels=stage1_n_components_range, xticks=[], title=&quot;Connectivity&quot;, ) # euclidean, morphology ax = axs[0, 1] plot_results = results[ (results[&quot;embedding&quot;] == &quot;morphology&quot;) &amp; (results[&quot;metric&quot;] == &quot;euclidean&quot;) ].pivot(**pivot_kws) sns.heatmap(plot_results.values, ax=ax, **heatmap_kws) ax.set( xticks=[], yticks=[], title=&quot;Morphology&quot;, ) # euclidean, MASE ax = axs[0, 2] plot_results = results[ (results[&quot;metric&quot;] == &quot;euclidean&quot;) &amp; (results[&quot;embedding&quot;] == &quot;joint&quot;) ].pivot(**pivot_kws) sns.heatmap(plot_results.values, ax=ax, **heatmap_kws) ax.set( xlabel=&quot;&quot;, yticks=[], xticks=[], title=&quot;Joint (MASE)&quot;, ) # cosine, connectivity ax = axs[1, 0] plot_results = results[ (results[&quot;embedding&quot;] == &quot;connectivity&quot;) &amp; (results[&quot;metric&quot;] == &quot;cosine&quot;) ].pivot(**pivot_kws) sns.heatmap(plot_results.values, ax=ax, **heatmap_kws) ax.set( ylabel=&quot;Metric = cosine n nStage 1 # components&quot;, yticklabels=stage1_n_components_range, xticks=[], ) # cosine, morphology ax = axs[1, 1] morph_cos_results = results[ (results[&quot;embedding&quot;] == &quot;morphology&quot;) &amp; (results[&quot;metric&quot;] == &quot;cosine&quot;) ].pivot(**pivot_kws) sns.heatmap(plot_results.values, ax=ax, **heatmap_kws) ax.set(xticks=[], yticks=[]) # cosine, MASE ax = axs[1, 2] plot_results = results[ (results[&quot;metric&quot;] == &quot;cosine&quot;) &amp; (results[&quot;embedding&quot;] == &quot;joint&quot;) ].pivot(**pivot_kws) sns.heatmap(plot_results, ax=ax, **heatmap_kws) ax.set( ylabel=&quot;&quot;, xlabel=&quot;Stage 2 # components&quot;, xticklabels=stage2_n_components_range, yticks=[], ) fig.suptitle(&quot;Discriminability&quot;, y=0.97) stashfig(&quot;discrim-by-dimensionality&quot;) . . # collapse print(f&quot;{(time.time() - t0)/60:.3f} minutes elapsed for whole notebook.&quot;) . . 5.942 minutes elapsed for whole notebook. .",
            "url": "https://neurodata.github.io/notebooks/pedigo/maggot/graspologic/2020/12/15/joint-connectivity-morphology.html",
            "relUrl": "/pedigo/maggot/graspologic/2020/12/15/joint-connectivity-morphology.html",
            "date": " • Dec 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Embedding graphs with covariates",
            "content": "Preliminaries . # collapse import os from pathlib import Path import matplotlib.pyplot as plt import networkx as nx import numpy as np import pandas as pd import seaborn as sns from matplotlib.lines import Line2D from sklearn.metrics import accuracy_score from sklearn.model_selection import cross_val_score from sklearn.neighbors import KNeighborsClassifier from umap import UMAP from graspologic.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed, selectSVD from graspologic.plot import pairplot, screeplot from graspologic.utils import get_lcc, pass_to_ranks, to_laplace from src.io import savefig from src.visualization import adjplot, matrixplot, set_theme set_theme() FNAME = os.path.basename(__file__)[:-3] def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, **kws) . . # collapse data_dir = Path(&quot;maggot_models/data/raw/OneDrive_1_10-21-2020&quot;) covariate_loc = data_dir / &quot;product_node_embedding.csv&quot; edges_loc = data_dir / &quot;product_edges.csv&quot; category_loc = data_dir / &quot;partition_mapping.csv&quot; covariate_df = pd.read_csv(covariate_loc, index_col=0, header=None).sort_index() meta_df = pd.read_csv(category_loc, index_col=0).sort_index() assert (covariate_df.index == meta_df.index).all() edges_df = pd.read_csv(edges_loc).sort_index() g = nx.from_pandas_edgelist(edges_df, edge_attr=&quot;weight&quot;, create_using=nx.DiGraph) adj = nx.to_numpy_array(g, nodelist=meta_df.index) . . # collapse print(f&quot;Number of vertices (original): {len(adj)}&quot;) make_lcc = False if make_lcc: adj, keep_inds = get_lcc(adj, return_inds=True) print(f&quot;Number of vertices (lcc): {len(adj)}&quot;) else: # HACK need to throw out some entire classes here that have very few members y = meta_df[&quot;cat_id&quot;] unique, inv, count = np.unique(y, return_inverse=True, return_counts=True) low_count = count &lt; 5 print(f&quot;Removing categories with fewer than 5 examples: {unique[low_count]}&quot;) keep_inds = ~np.isin(inv, unique[low_count]) adj = adj[np.ix_(keep_inds, keep_inds)] print(f&quot;Number of vertices (small classes removed): {len(adj)}&quot;) meta_df = meta_df.iloc[keep_inds] covariate_df = covariate_df.iloc[keep_inds] Y = covariate_df.values . . Number of vertices (original): 1035 Removing categories with fewer than 5 examples: [3 4 6 7] Number of vertices (small classes removed): 1029 . # collapse colors = sns.color_palette(&quot;deep&quot;) palette = dict(zip(np.unique(meta_df[&quot;cat_id&quot;]), colors)) . . Adjacency matrix (sorted by category) . # collapse adjplot( pass_to_ranks(adj), plot_type=&quot;scattermap&quot;, meta=meta_df, colors=&quot;cat_id&quot;, sort_class=&quot;cat_id&quot;, palette=palette, title=r&quot;Adjacency matrix ($A$)&quot;, ) . . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee60d49150&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7fee7a5f9650&gt;, &lt;matplotlib.axes._axes.Axes at 0x7fee610c9950&gt;, &lt;matplotlib.axes._axes.Axes at 0x7fee61106410&gt;) . ax = screeplot( pass_to_ranks(adj), show_first=40, cumulative=False, title=&quot;Adjacency scree plot&quot; ) . Covariates (sorted by category) . # collapse matrixplot( Y, row_meta=meta_df, row_colors=&quot;cat_id&quot;, row_sort_class=&quot;cat_id&quot;, row_palette=palette, title=r&quot;Metadata ($X$)&quot;, ) . . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee6278fd90&gt;, &lt;mpl_toolkits.axes_grid1.axes_divider.AxesDivider at 0x7fee627bcc90&gt;, &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee6278fd90&gt;, &lt;matplotlib.axes._axes.Axes at 0x7fee62783e10&gt;) . # collapse ax = screeplot(Y, show_first=40, cumulative=False, title=&quot;Covariate scree plot&quot;) . . R-LSE . # collapse lse = LaplacianSpectralEmbed(form=&quot;R-DAD&quot;) embedding = lse.fit_transform(pass_to_ranks(adj)) pairplot(embedding[0], labels=meta_df[&quot;cat_id&quot;].values, palette=palette) stashfig(&quot;pairplot-rlse&quot;) . . /Users/bpedigo/JHU_code/maggot_models/graspologic/graspologic/embed/lse.py:161: UserWarning: Input graph is not fully connected. Results may notbe optimal. You can compute the largest connected component byusing ``graspologic.utils.get_lcc``. Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/pairplot-rlse.png . # collapse concat_embedding = np.concatenate(embedding, axis=1) umapper = UMAP(min_dist=0.7, metric=&quot;cosine&quot;) umap_embedding = umapper.fit_transform(concat_embedding) plot_df = pd.DataFrame( data=umap_embedding, columns=[f&quot;umap_{i}&quot; for i in range(umap_embedding.shape[1])], index=meta_df.index, ) plot_df[&quot;cat_id&quot;] = meta_df[&quot;cat_id&quot;] fig, ax = plt.subplots(1, 1, figsize=(10, 10)) sns.scatterplot( data=plot_df, x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=20, alpha=0.7, hue=&quot;cat_id&quot;, palette=palette, ax=ax, ) ax.get_legend().remove() ax.legend(bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;) ax.set_title(&quot;UMAP o R-LSE&quot;) ax.axis(&quot;off&quot;) stashfig(&quot;umap-rlse&quot;) . . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/umap-rlse.png . CASE . Running CASE with a few different parameters . # collapse L = to_laplace(pass_to_ranks(adj), form=&quot;R-DAD&quot;) # D_{tau}^{-1/2} A D_{tau}^{-1/2} Y = covariate_df.values def build_case_matrix(L, Y, alpha, method=&quot;assort&quot;): if method == &quot;assort&quot;: L_case = L + alpha * Y @ Y.T elif method == &quot;nonassort&quot;: L_case = L @ L.T + alpha * Y @ Y.T elif method == &quot;cca&quot;: # doesn&#39;t make sense here, I don&#39;t thinks L_case = L @ Y return L_case def fit_case(L, Y, alpha, method=&quot;assort&quot;, n_components=None): L_case = build_case_matrix(L, Y, alpha, method=method) case_embedder = AdjacencySpectralEmbed( n_components=n_components, check_lcc=False, diag_aug=False, concat=True ) case_embedding = case_embedder.fit_transform(L_case) return case_embedding n_components = 8 alphas = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06] methods = [&quot;assort&quot;, &quot;nonassort&quot;] case_by_params = {} umap_by_params = {} for method in methods: for alpha in alphas: case_embedding = fit_case(L, Y, alpha, method=method, n_components=n_components) umapper = UMAP(min_dist=0.7, metric=&quot;cosine&quot;) umap_embedding = umapper.fit_transform(case_embedding) case_by_params[(method, alpha)] = case_embedding umap_by_params[(method, alpha)] = umap_embedding . . Plot each of the embeddings . # collapse fig, axs = plt.subplots( len(methods), len(alphas[:4]), figsize=5 * np.array([len(alphas[:4]), len(methods)]) ) for i, method in enumerate(methods): for j, alpha in enumerate(alphas[:4]): ax = axs[i, j] umap_embedding = umap_by_params[(method, alpha)] plot_df = pd.DataFrame( data=umap_embedding, columns=[f&quot;umap_{c}&quot; for c in range(umap_embedding.shape[1])], index=meta_df.index, ) plot_df[&quot;cat_id&quot;] = meta_df[&quot;cat_id&quot;] sns.scatterplot( data=plot_df, x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=20, alpha=0.7, hue=&quot;cat_id&quot;, palette=&quot;deep&quot;, ax=ax, ) ax.get_legend().remove() # ax.axis(&quot;off&quot;) ax.set(xticks=[], yticks=[], ylabel=&quot;&quot;, xlabel=&quot;&quot;) ax.set_title(r&quot;$ alpha$ = &quot; + f&quot;{alpha}&quot;) axs[0, -1].legend(bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;, title=&quot;Category&quot;) axs[0, 0].set_ylabel(&quot;CASE (assortative)&quot;) axs[1, 0].set_ylabel(&quot;CASE (non-assortative)&quot;) fig.suptitle(&quot;UMAP o CASE embeddings&quot;) stashfig(&quot;casc-umaps&quot;) . . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/casc-umaps.png . MASE . Running MASE (here I&#39;m using the Laplacian and the covariates as the two inputs) . # collapse Y = covariate_df.values n_components = 6 # TODO picked this just roughly looking at screeplots U_Y, D_Y, Vt_Y = selectSVD(Y @ Y.T, n_components=n_components) U_L, D_L, Vt_L = selectSVD(L, n_components=n_components) covariate_embedding = U_Y concatenated_latent = np.concatenate((U_L, U_Y), axis=1) U_joint, D_joint, Vt_joint = selectSVD(concatenated_latent, n_components=8) mase_embedding = U_joint . . A simple classifier on the embeddings . Here I just do a simple 5-nearest-neighbors classifier. Note that I haven&#39;t done any tuning of the classifier (like how many neighbors to use or distances other than Euclidean) or the dimension of the embedding itself. . In the experiment below, I am doing cross validation, and not treating the out-of-graph nodes any differently (i.e. they are just mixed in for the cross validation). . # collapse classifier = KNeighborsClassifier(n_neighbors=5) y = meta_df[&quot;cat_id&quot;].values rows = [] for method in methods: for alpha in alphas: X = case_by_params[(method, alpha)] cval_scores = cross_val_score(classifier, X, y=y) for score in cval_scores: rows.append({&quot;score&quot;: score, &quot;alpha&quot;: alpha, &quot;method&quot;: method}) X = mase_embedding cval_scores = cross_val_score(classifier, X, y) for score in cval_scores: rows.append({&quot;score&quot;: score, &quot;alpha&quot;: alpha + 0.01, &quot;method&quot;: &quot;MASE&quot;}) results = pd.DataFrame(rows) . . # collapse x_range = np.array(alphas) x_half_bin = 0.5 * (x_range[1] - x_range[0]) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) results[&quot;jitter_alpha&quot;] = results[&quot;alpha&quot;] + np.random.uniform( -0.0025, 0.0025, size=len(results) ) method_palette = dict(zip(np.unique(results[&quot;method&quot;]), colors)) sns.lineplot( x=&quot;alpha&quot;, hue=&quot;method&quot;, y=&quot;score&quot;, data=results, ax=ax, ci=None, palette=method_palette, ) sns.scatterplot( x=&quot;jitter_alpha&quot;, hue=&quot;method&quot;, y=&quot;score&quot;, data=results, ax=ax, palette=method_palette, s=30, ) ax.set( ylabel=&quot;Accuracy&quot;, xlabel=r&quot;$ alpha$ (CASE tuning parameter)&quot;, title=&quot;5-fold cross validation, KNN (with isolates)&quot;, ) ax.xaxis.set_major_locator(plt.FixedLocator(alphas)) ax.xaxis.set_major_formatter(plt.FixedFormatter(alphas)) ax.axvline(0.065, color=&quot;grey&quot;, alpha=0.7) mean_mase = results[results[&quot;method&quot;] == &quot;MASE&quot;][&quot;score&quot;].mean() ax.plot( [0.067, 0.073], [mean_mase, mean_mase], color=method_palette[&quot;MASE&quot;], ) handles, labels = ax.get_legend_handles_labels() handles = handles[4:] labels = labels[4:] labels[0] = &quot;Method&quot; labels[1] = r&quot;CASE$_{a}$&quot; labels[2] = r&quot;CASE$_{na}$&quot; handles.append(Line2D([0], [0], color=&quot;black&quot;)) labels.append(&quot;Mean&quot;) ax.get_legend().remove() ax.legend( bbox_to_anchor=( 1, 1, ), loc=&quot;upper left&quot;, handles=handles, labels=labels, ) for i, x in enumerate(x_range): if i % 2 == 0: ax.axvspan( x - x_half_bin, x + x_half_bin, color=&quot;lightgrey&quot;, alpha=0.3, linewidth=0, zorder=-1, ) stashfig(f&quot;knn-lcc={make_lcc}&quot;) . . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/knn-lcc=False.png . Using only the nodes with graph signal as training data . Here I just pick one of the parameter sets for the CASE embedding from above, as well as the MASE embedding and the embedding we get for just the covariates alone. Then I use all in-graph nodes as training data, and ask how well they predict for the out-of-graph nodes. . # collapse _, lcc_inds = get_lcc(adj, return_inds=True) not_lcc_inds = np.setdiff1d(np.arange(len(adj)), lcc_inds) y = meta_df[&quot;cat_id&quot;].values y_train = y[lcc_inds] y_test = y[not_lcc_inds] # just pick one CASE embedding method = &quot;assort&quot; alpha = 0.02 case_embedding = case_by_params[(method, alpha)] def classify_out_of_graph(X): classifier = KNeighborsClassifier(n_neighbors=5) X_train = X[lcc_inds] X_test = X[not_lcc_inds] classifier.fit(X_train, y_train) y_pred = classifier.predict(X_test) score = accuracy_score(y_test, y_pred) return score, y_pred def plot_out_of_graph(embedding, score, incorrect, method=&quot;&quot;): incorrect = y_test != y_pred umapper = UMAP( n_neighbors=10, min_dist=0.8, metric=&quot;cosine&quot;, negative_sample_rate=30 ) umap_embedding = umapper.fit_transform(embedding) plot_df = pd.DataFrame( data=umap_embedding, columns=[f&quot;umap_{c}&quot; for c in range(umap_embedding.shape[1])], index=meta_df.index, ) plot_df[&quot;cat_id&quot;] = meta_df[&quot;cat_id&quot;] plot_df[&quot;in_lcc&quot;] = False plot_df.loc[plot_df.index[lcc_inds], &quot;in_lcc&quot;] = True plot_df[&quot;correct&quot;] = True plot_df.loc[plot_df.index[not_lcc_inds[incorrect]], &quot;correct&quot;] = False fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.scatterplot( data=plot_df[plot_df[&quot;in_lcc&quot;]], x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=20, alpha=0.2, hue=&quot;cat_id&quot;, palette=palette, ax=ax, ) markers = dict(zip([True, False], [&quot;o&quot;, &quot;X&quot;])) sns.scatterplot( data=plot_df[~plot_df[&quot;in_lcc&quot;]], x=&quot;umap_0&quot;, y=&quot;umap_1&quot;, s=30, alpha=0.9, hue=&quot;cat_id&quot;, palette=palette, style=&quot;correct&quot;, markers=markers, ax=ax, ) ax.get_legend().remove() correct_line = Line2D( [0], [0], color=&quot;black&quot;, lw=0, marker=&quot;o&quot;, mew=0, markersize=7 ) incorrect_line = Line2D( [0], [0], color=&quot;black&quot;, lw=0, marker=&quot;X&quot;, mew=0, markersize=7 ) lines = [correct_line, incorrect_line] labels = [&quot;Correct&quot;, &quot;Incorrect&quot;] ax.legend(lines, labels) ax.axis(&quot;off&quot;) ax.set_title(f&quot;{method}, predictions on isolates: accuracy {score:.2f}&quot;) return fig, ax . . score, y_pred = classify_out_of_graph(case_embedding) plot_out_of_graph(case_embedding, score, y_pred, method=&quot;CASE&quot;) stashfig(&quot;case-isolate-predictions-umap&quot;) . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/case-isolate-predictions-umap.png . score, y_pred = classify_out_of_graph(mase_embedding) plot_out_of_graph(mase_embedding, score, y_pred, method=&quot;MASE&quot;) stashfig(&quot;mase-isolate-predictions-umap&quot;) . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/mase-isolate-predictions-umap.png . score, y_pred = classify_out_of_graph(U_Y) plot_out_of_graph(U_Y, score, y_pred, method=&quot;Covariates&quot;) stashfig(&quot;covariates-isolate-predictions-umap&quot;) . Saved figure to maggot_models/notebooks/outs/194.2-BDP-not-a-brain/figs/covariates-isolate-predictions-umap.png .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/11/17/covariate-embedding.html",
            "relUrl": "/pedigo/graspologic/2020/11/17/covariate-embedding.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post4": {
            "title": "Finding the right categorical labeling",
            "content": "Often when we have data that can be binned into discrete groups (clusters) we represent this with a label vector $y$. Usually we just make up a labeling scheme such as assigning the integers $0,...,K-1$ to represent $K$ different clusters. The $i$th element of the label vector denotes which group or cluster the $i$th sample belongs to, e.g. if the $i$th element is a $0$, then data point $i$ belongs to cluster $0$. . Many unsupervised clustering techniques seek to uncover this labeling (predict $y$). Let&#39;s imagine we ran a clustering algorithm, and it predicted the true clusters perfectly. All this means is that for each true cluster $0,...,K-1$, all of the points belonging to the same cluster in $y$ also belong in the same cluster in the predicted vector $ hat{y}$. However, if we are using an unsupervised clustering algorithm, it is unlikely that cluster $0$ in $y$ corresponds to cluster $0$ in $ hat{y}$, and so forth for all of the other clusters. There is no way for an unsupervised method to know this arbitrary labeling that we made up for the clusters, so it just made up an arbitrary labeling scheme too! . Below I present a simple algorithm that will often &quot;remap&quot; the categorical labeling in $ hat{y}$ to match $y$. It won&#39;t always work perfectly - for instance if two clusters in $ hat{y}$ both contain exactly half of a cluster from $y$, there is no way to resolve this ambiguity fairly. Still, it is often useful in practice. . import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.datasets import make_blobs from sklearn.mixture import GaussianMixture from sklearn.metrics import confusion_matrix from scipy.optimize import linear_sum_assignment sns.set_context(&quot;talk&quot;) X, y_true = make_blobs(n_samples=100, random_state=888888) n_classes = len(np.unique(y_true)) palette = dict(zip(np.arange(n_classes), sns.color_palette(&quot;deep&quot;, n_classes))) plot_df = pd.DataFrame(data=X, columns=np.arange(X.shape[1], dtype=str)) plot_df[&quot;true_labels&quot;] = y_true def simple_scatter(ax, hue, title=&quot;&quot;): sns.scatterplot(data=plot_df, x=&quot;0&quot;, y=&quot;1&quot;, hue=hue, ax=ax, palette=palette) ax.set(xticks=[], yticks=[], ylabel=&quot;&quot;, xlabel=&quot;&quot;, title=title) ax.get_legend().remove() fig, axs = plt.subplots(1, 3, figsize=(18, 6)) simple_scatter(axs[0], &quot;true_labels&quot;, title=&quot;Known labeling&quot;) gmm = GaussianMixture(n_components=3, random_state=80808) y_predicted = gmm.fit_predict(X) plot_df[&quot;predicted_labels&quot;] = y_predicted simple_scatter(axs[1], &quot;predicted_labels&quot;, title=&quot;Predicted labeling (original)&quot;) def remap_labels(y_true, y_pred, return_map=False): &quot;&quot;&quot; Remaps a categorical labeling (such as one predicted by a clustering algorithm) to match the labels used by another similar labeling. Parameters - y_true : array-like of shape (n_samples,) Ground truth labels, or, labels to map to. y_pred : array-like of shape (n_samples,) Labels to remap to match the categorical labeling of `y_true`. Returns - remapped_y_pred : np.ndarray of shape (n_samples,) Same categorical labeling as that of `y_pred`, but with the category labels permuted to best match those of `y_true`. label_map : dict Mapping from the original labels of `y_pred` to the new labels which best resemble those of `y_true`. Examples -- &gt;&gt;&gt; y_true = np.array([0,0,1,1,2,2]) &gt;&gt;&gt; y_pred = np.array([2,2,1,1,0,0]) &gt;&gt;&gt; remap_labels(y_true, y_pred) array([0, 0, 1, 1, 2, 2]) &quot;&quot;&quot; confusion_mat = confusion_matrix(y_true, y_pred) row_inds, col_inds = linear_sum_assignment(confusion_mat, maximize=True) label_map = dict(zip(col_inds, row_inds)) remapped_y_pred = np.vectorize(label_map.get)(y_pred) if return_map: return remapped_y_pred, label_map else: return remapped_y_pred y_remapped = remap_labels(y_true, y_predicted) plot_df[&quot;remapped_labels&quot;] = y_remapped simple_scatter(axs[2], &quot;remapped_labels&quot;, title=&quot;Predicted labeling (remapped)&quot;) handles, labels = axs[1].get_legend_handles_labels() _ = axs[1].legend( handles[:], labels[:], bbox_to_anchor=(0.5, 0), loc=&quot;upper center&quot;, ncol=3, title=&quot;Cluster labels&quot;, ) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/clustering/2020/10/12/remapping-labels.html",
            "relUrl": "/pedigo/graspologic/clustering/2020/10/12/remapping-labels.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Modeling the distribution of neural networks",
            "content": "# collapse import os import warnings import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from graspy.plot import heatmap, pairplot from matplotlib.transforms import blended_transform_factory from scipy.stats import pearsonr, spearmanr from sklearn.datasets import load_digits from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier from graspologic.embed import AdjacencySpectralEmbed, select_dimension from graspologic.match import GraphMatch from src.io import savefig from src.visualization import set_theme FNAME = os.path.basename(__file__)[:-3] set_theme() np.random.seed(8888) def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, print_out=False, **kws) . . Trying to learn distributions on NNs trained on the same task . Example training and predicted labels . Adapted from Sklearn docs . # collapse # The digits dataset digits = load_digits() # The data that we are interested in is made of 8x8 images of digits, let&#39;s # have a look at the first 4 images, stored in the `images` attribute of the # dataset. If we were working from image files, we could load them using # matplotlib.pyplot.imread. Note that each image must have the same size. For these # images, we know which digit they represent: it is given in the &#39;target&#39; of # the dataset. _, axes = plt.subplots(2, 4) images_and_labels = list(zip(digits.images, digits.target)) for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) ax.set_title(&quot;Training: %i&quot; % label, fontsize=&quot;x-small&quot;) # To apply a classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) # Create a classifier: a support vector classifier # classifier = svm.SVC(gamma=0.001) classifier = MLPClassifier() # Split data into train and test subsets X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.5, shuffle=False ) # We learn the digits on the first half of the digits classifier.fit(X_train, y_train) # Now predict the value of the digit on the second half: predicted = classifier.predict(X_test) images_and_predictions = list(zip(digits.images[n_samples // 2 :], predicted)) for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&quot;nearest&quot;) ax.set_title(&quot;Prediction: %i&quot; % prediction, fontsize=&quot;x-small&quot;) plt.show() . . Training multiple NNs on the same task . # collapse hidden_layer_sizes = (15,) n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) n_replicates = 8 adjs = [] all_biases = [] all_weights = [] accuracies = [] mlps = [] for i in range(n_replicates): X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.3, shuffle=True ) mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=600) mlp.fit(X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) weights_by_layer = mlp.coefs_ all_biases.append(mlp.intercepts_) all_weights.append(mlp.coefs_) accuracies.append(acc) mlps.append(mlp) print(f&quot;Test accuracy score for NN {i+1}: {acc}&quot;) n_nodes = 0 for weights in weights_by_layer: n_source, n_target = weights.shape n_nodes += n_source n_nodes += n_target n_nodes += len(hidden_layer_sizes) + 1 adj = np.zeros((n_nodes, n_nodes)) n_nodes_visited = 0 for i, weights in enumerate(weights_by_layer): n_source, n_target = weights.shape adj[ n_nodes_visited : n_nodes_visited + n_source, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target, ] = weights adj[ -i - 1, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target ] = mlp.intercepts_[i] n_nodes_visited += n_source adjs.append(adj) all_biases = [np.concatenate(b) for b in all_biases] all_biases = np.stack(all_biases).T all_weights = [[w.ravel() for w in weights] for weights in all_weights] all_weights = [np.concatenate(w) for w in all_weights] all_weights = np.stack(all_weights).T . . Test accuracy score for NN 1: 0.95 Test accuracy score for NN 2: 0.9518518518518518 Test accuracy score for NN 3: 0.9648148148148148 Test accuracy score for NN 4: 0.9648148148148148 Test accuracy score for NN 5: 0.9462962962962963 Test accuracy score for NN 6: 0.9351851851851852 Test accuracy score for NN 7: 0.9685185185185186 Test accuracy score for NN 8: 0.9462962962962963 . Plotting the adjacency matrices for each NN . # collapse vmax = max(map(np.max, adjs)) vmin = min(map(np.min, adjs)) fig, axs = plt.subplots(2, 4, figsize=(20, 10)) for i, ax in enumerate(axs.ravel()): heatmap(adjs[i], cbar=False, vmin=vmin, vmax=vmax, ax=ax, title=f&quot;NN {i + 1}&quot;) fig.suptitle(&quot;Adjacency matrices&quot;, fontsize=&quot;large&quot;, fontweight=&quot;bold&quot;) plt.tight_layout() stashfig(&quot;multi-nn-adjs&quot;) . . Trying to model the weights with something simple . I make a new NN where the weights are set to the simple average of weights across all of the NN that I fit and see how it performs. . # collapse adj_bar = np.mean(np.stack(adjs), axis=0) def mlp_from_adjacency(adj, X_train, y_train): mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1) with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;, category=UserWarning) mlp.fit( X_train, y_train ) # dummy fit, just to set parameters like shape of input/output n_nodes_visited = 0 for i, weights in enumerate(mlp.coefs_): n_source, n_target = weights.shape mlp.coefs_[i] = adj[ n_nodes_visited : n_nodes_visited + n_source, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target, ] mlp.intercepts_[i] = adj[ -i - 1, n_nodes_visited + n_source : n_nodes_visited + n_source + n_target ] n_nodes_visited += n_source return mlp X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.7, shuffle=True ) mlp = mlp_from_adjacency(adj_bar, X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) print(f&quot;Test accuracy score for NN with mean weights: {acc}&quot;) . . Test accuracy score for NN with mean weights: 0.27106518282988873 . Why I think this doesn&#39;t work, and what we might be able to do about it . There are two big issues as I see it: . Permuation nonidentifiability in the model | Lack of edge-edge dependence structure in our models . Below I investigate the first issue, haven&#39;t thought about what to do for the second . | . Plotting the learned weights against each other . If each network has $d$ free weight parameters, and there are $T$ of them, I form the $T$ by $d$ matrix of weights per neural network, and then plot each network&#39;s weights against each other. . # collapse def corrplot(x, y, *args, ax=None, fontsize=&quot;xx-small&quot;, **kwargs): if ax is None: ax = plt.gca() pearsons, _ = pearsonr(x, y) spearmans, _ = spearmanr(x, y) text = r&quot;$ rho_p: $&quot; + f&quot;{pearsons:.3f} n&quot; text += r&quot;$ rho_s: $&quot; + f&quot;{spearmans:.3f}&quot; ax.text(1, 1, text, ha=&quot;right&quot;, va=&quot;top&quot;, transform=ax.transAxes, fontsize=fontsize) pg = pairplot( all_weights, alpha=0.1, title=&quot;Weights&quot;, col_names=[f&quot;NN {i+1}&quot; for i in range(all_weights.shape[1])], ) pg.map_offdiag(corrplot) stashfig( &quot;weight-pairplot&quot;, ) . . Can graph matching fix the permutation nonidentifiability? . Given one neural network architecture, one could permute the labels/orders of the hidden units, and the network would be functionally equivalent. This means that when comparing the architectures of two learned neural networks against each other, there is a nonidentifiability problem caused by this arbitrary permutation. Even if we imagine that two neural networks learned the exact same weights, they are unlikely to look similar at a glance because it is unlikely they learned the same weights and the same permutation. Let&#39;s see if graph matching can help resolve this nonidentifiability. . # collapse heatmap_kws = dict(vmin=vmin, vmax=vmax, cbar=False) fig, axs = plt.subplots(1, 3, figsize=(12, 4)) heatmap(adjs[0], ax=axs[0], title=&quot;NN 1 pre-GM&quot;, **heatmap_kws) heatmap(adjs[1], ax=axs[1], title=&quot;NN 2 pre-GM&quot;, **heatmap_kws) heatmap(adjs[0] - adjs[1], ax=axs[2], title=&quot;Difference&quot;, **heatmap_kws) stashfig(&quot;pre-gm-adjs&quot;) seeds = np.concatenate( ( np.arange(data.shape[1]), np.arange(len(adj) - 10 - len(hidden_layer_sizes) - 1, len(adj)), ) ) gm = GraphMatch(n_init=20, init_method=&quot;barycenter&quot;) gm.fit(adjs[0], adjs[1], seeds_A=seeds, seeds_B=seeds) perm_inds = gm.perm_inds_ adj_1_matched = adjs[1][np.ix_(perm_inds, perm_inds)].copy() fig, axs = plt.subplots(1, 3, figsize=(12, 4)) heatmap(adjs[0], ax=axs[0], title=&quot;NN 1 post-GM&quot;, **heatmap_kws) heatmap(adj_1_matched, ax=axs[1], title=&quot;NN 2 post-GM&quot;, **heatmap_kws) heatmap(adjs[0] - adj_1_matched, ax=axs[2], title=&quot;Difference&quot;, **heatmap_kws) stashfig(&quot;post-gm-adjs&quot;) fig, axs = plt.subplots(1, 2, figsize=(16, 8)) ax = axs[0] sns.scatterplot(adjs[0].ravel(), adjs[1].ravel(), ax=ax, alpha=0.3, linewidth=0, s=15) corrplot(adjs[0].ravel(), adjs[1].ravel(), ax=ax, fontsize=&quot;medium&quot;) ax.set( title=&quot;Weights pre-GM&quot;, xticks=[], yticks=[], xlabel=&quot;NN 1 weights&quot;, ylabel=&quot;NN 2 weights&quot;, ) ax.axis(&quot;equal&quot;) ax = axs[1] sns.scatterplot( adjs[0].ravel(), adj_1_matched.ravel(), ax=ax, alpha=0.3, linewidth=0, s=15 ) corrplot(adjs[0].ravel(), adj_1_matched.ravel(), ax=ax, fontsize=&quot;medium&quot;) ax.set( title=&quot;Weights post-GM&quot;, xticks=[], yticks=[], xlabel=&quot;NN 1 weights&quot;, ylabel=&quot;NN 2 weights&quot;, ) ax.axis(&quot;equal&quot;) stashfig(&quot;pre-post-weights-gm&quot;) . . Unraveling the nonidentifiability with GM . I match the weights in each network to the best performing one using graph matching. . NB: the way I&#39;m doing this is more convenient but probably dumb, really should just be matching on a per-hidden-layer basis. But in this case I have one hidden layer and the others are seeds so it doesn&#39;t matter. . # collapse best_model_ind = np.argmax(accuracies) best_adj = adjs[best_model_ind] matched_adjs = [] for i, adj in enumerate(adjs): gm = GraphMatch(n_init=20, init_method=&quot;barycenter&quot;) gm.fit(best_adj, adj, seeds_A=seeds, seeds_B=seeds) perm_inds = gm.perm_inds_ matched_adj = adj[np.ix_(perm_inds, perm_inds)].copy() matched_adjs.append(matched_adj) . . All pairwise weight comparisons after matching . We see that the correlation in weights improves somewhat, though they are still not highly correlated. . # collapse all_matched_weights = [a.ravel() for a in matched_adjs] all_matched_weights = np.stack(all_matched_weights, axis=1) all_matched_weights = all_matched_weights[ np.linalg.norm(all_matched_weights, axis=1) != 0, : ] all_matched_weights.shape pg = pairplot( all_matched_weights, alpha=0.1, title=&quot;Matched weights&quot;, col_names=[f&quot;NN {i+1}&quot; for i in range(all_matched_weights.shape[1])], ) pg.map_offdiag(corrplot) stashfig( &quot;matched-weight-pairplot&quot;, ) . . Using $ bar{A}_{matched}$ as the weight matrix . I take the mean of the weights after matching, and ask how well this mean weight matrix performs when converted back to a neural net. . # collapse matched_adj_bar = np.mean(np.stack(matched_adjs), axis=0) X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.7, shuffle=True ) mlp = mlp_from_adjacency(matched_adj_bar, X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) print(f&quot;Test accuracy score for NN with average adjacency (matched): {acc}&quot;) . . Test accuracy score for NN with average adjacency (matched): 0.7090620031796503 . Decomposing the matched and unmatched adjacency matrices . # collapse def embed(A): ase = AdjacencySpectralEmbed(n_components=len(A), algorithm=&quot;full&quot;) X, Y = ase.fit_transform(A) elbow_inds, _ = select_dimension(A, n_elbows=4) elbow_inds = np.array(elbow_inds) return X, Y, ase.singular_values_, elbow_inds def screeplot(sing_vals, elbow_inds, color=None, ax=None, label=None, linestyle=&quot;-&quot;): if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 4)) plt.plot( range(1, len(sing_vals) + 1), sing_vals, color=color, label=label, linestyle=linestyle, ) plt.scatter( elbow_inds, sing_vals[elbow_inds - 1], marker=&quot;x&quot;, s=50, zorder=10, color=color, ) ax.set(ylabel=&quot;Singular value&quot;, xlabel=&quot;Index&quot;) return ax X_matched, Y_matched, sing_vals_matched, elbow_inds_matched = embed(matched_adj_bar) X_unmatched, Y_unmatched, sing_vals_unmatched, elbow_inds_unmatched = embed(adj_bar) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) screeplot(sing_vals_matched, elbow_inds_matched, ax=ax, label=&quot;matched&quot;) screeplot( sing_vals_unmatched, elbow_inds_unmatched, ax=ax, label=&quot;unmatched&quot;, linestyle=&quot;--&quot; ) ax.legend() stashfig(&quot;screeplot-adj-bars&quot;) . . Plotting accuracy as a function of adjacency rank . # collapse match_latent_map = { &quot;matched&quot;: (X_matched, Y_matched), &quot;unmatched&quot;: (X_unmatched, Y_unmatched), } n_components_range = np.unique( np.geomspace(1, len(matched_adj_bar) + 1, num=10, dtype=int) ) rows = [] n_resamples = 8 for resample in range(n_resamples): for n_components in n_components_range: X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.7, shuffle=True ) for method in [&quot;matched&quot;, &quot;unmatched&quot;]: X, Y = match_latent_map[method] low_rank_adj = X[:, :n_components] @ Y[:, :n_components].T mlp = mlp_from_adjacency(low_rank_adj, X_train, y_train) y_pred = mlp.predict(X_test) acc = accuracy_score(y_test, y_pred) rows.append({&quot;accuracy&quot;: acc, &quot;rank&quot;: n_components, &quot;type&quot;: method}) results = pd.DataFrame(rows) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.lineplot( x=&quot;rank&quot;, y=&quot;accuracy&quot;, data=results, style=&quot;type&quot;, hue=&quot;type&quot;, ax=ax, # markers=[&quot;o&quot;, &quot;o&quot;], ) results[&quot;jitter_rank&quot;] = results[&quot;rank&quot;] + np.random.uniform( -1, 1, size=len(results[&quot;rank&quot;]) ) sns.scatterplot( x=&quot;jitter_rank&quot;, y=&quot;accuracy&quot;, data=results, hue=&quot;type&quot;, ax=ax, s=10, legend=False ) ax.set(yticks=[0.2, 0.4, 0.6, 0.8], ylim=(0, 0.82), xlabel=&quot;Rank&quot;, ylabel=&quot;Accuracy&quot;) ax.axhline(1 / 10, linestyle=&quot;:&quot;, linewidth=1.5, color=&quot;black&quot;) ax.text( 1, 1 / 10, &quot;Chance&quot;, ha=&quot;right&quot;, va=&quot;bottom&quot;, transform=blended_transform_factory(ax.transAxes, ax.transData), fontsize=&quot;small&quot;, ) stashfig(&quot;acc-by-rank&quot;) . .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/10/05/nn-distributions.html",
            "relUrl": "/pedigo/graspologic/2020/10/05/nn-distributions.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Maggot embedding alignment",
            "content": "# collapse import os import time import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.stats import ortho_group from sklearn.neighbors import NearestNeighbors import graspologic as gl from graspologic.align import OrthogonalProcrustes, SeedlessProcrustes from graspologic.embed import AdjacencySpectralEmbed, select_dimension from graspologic.plot import pairplot from graspologic.utils import augment_diagonal, pass_to_ranks from src.data import load_metagraph from src.graph import MetaGraph from src.io import savefig from src.visualization import adjplot, set_theme print(f&quot;graspologic version: {gl.__version__}&quot;) print(f&quot;seaborn version: {sns.__version__}&quot;) set_theme() palette = dict(zip([&quot;Left&quot;, &quot;Right&quot;, &quot;OP&quot;, &quot;SP&quot;], sns.color_palette(&quot;Set1&quot;))) FNAME = os.path.basename(__file__)[:-3] def stashfig(name, **kws): savefig(name, foldername=FNAME, save_on=True, fmt=&quot;pdf&quot;, **kws) savefig(name, foldername=FNAME, save_on=True, fmt=&quot;png&quot;, dpi=300, **kws) . . graspologic version: 0.1.0.dev20201001120300 seaborn version: 0.10.1 . Load in the data, use only the known pairs . # collapse mg = load_metagraph(&quot;G&quot;) pair_meta = pd.read_csv( &quot;maggot_models/experiments/graph_match/outs/pair_meta.csv&quot;, index_col=0 ) pair_key = &quot;pair_id&quot; pair_meta = pair_meta[pair_meta[f&quot;{pair_key[:-3]}&quot;].isin(pair_meta.index)] pair_meta = pair_meta.sort_values([&quot;hemisphere&quot;, pair_key]) mg = mg.reindex(pair_meta.index.values, use_ids=True) mg = MetaGraph(mg.adj, pair_meta) n_pairs = len(pair_meta) // 2 left_inds = np.arange(n_pairs) right_inds = left_inds.copy() + n_pairs left_mg = MetaGraph(mg.adj[np.ix_(left_inds, left_inds)], mg.meta.iloc[left_inds]) right_mg = MetaGraph(mg.adj[np.ix_(right_inds, right_inds)], mg.meta.iloc[right_inds]) assert (left_mg.meta[pair_key].values == right_mg.meta[pair_key].values).all() print(f&quot;Working with {n_pairs} pairs.&quot;) . . Working with 1173 pairs. . Plot the adjacency matrices, sorted by known pairs . # collapse fig, axs = plt.subplots(1, 2, figsize=(10, 5)) adjplot( left_mg.adj, plot_type=&quot;scattermap&quot;, sizes=(1, 2), ax=axs[0], title=r&quot;Left $ to$ left&quot;, color=palette[&quot;Left&quot;], ) adjplot( right_mg.adj, plot_type=&quot;scattermap&quot;, sizes=(1, 2), ax=axs[1], title=r&quot;Right $ to$ right&quot;, ) stashfig(&quot;left-right-adjs&quot;) . . Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/left-right-adjs.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/left-right-adjs.png . Embed the ipsilateral subgraphs . Computations are happening in n_components dimensions, where n_components = 8 (dashed line in screeplot) | In many plots, I only show the first 4 dimensions just for clarity | . # collapse def plot_latents(left, right, title=&quot;&quot;): plot_data = np.concatenate([left, right], axis=0) labels = np.array([&quot;Left&quot;] * len(left) + [&quot;Right&quot;] * len(right)) pg = pairplot(plot_data[:, :4], labels=labels, title=title) return pg def screeplot(sing_vals, elbow_inds, color=None, ax=None, label=None): if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 4)) plt.plot(range(1, len(sing_vals) + 1), sing_vals, color=color, label=label) plt.scatter( elbow_inds, sing_vals[elbow_inds - 1], marker=&quot;x&quot;, s=50, zorder=10, color=color ) ax.set(ylabel=&quot;Singular value&quot;, xlabel=&quot;Index&quot;) return ax def embed(adj, n_components=40): elbow_inds, elbow_vals = select_dimension( augment_diagonal(pass_to_ranks(adj)), n_elbows=4 ) elbow_inds = np.array(elbow_inds) ase = AdjacencySpectralEmbed(n_components=n_components) out_latent, in_latent = ase.fit_transform(pass_to_ranks(adj)) return out_latent, in_latent, ase.singular_values_, elbow_inds n_components = 8 max_n_components = 40 left_out_latent, left_in_latent, left_sing_vals, left_elbow_inds = embed( left_mg.adj, n_components=max_n_components ) right_out_latent, right_in_latent, right_sing_vals, right_elbow_inds = embed( right_mg.adj, n_components=max_n_components ) # plot the screeplot fig, ax = plt.subplots(1, 1, figsize=(8, 4)) screeplot(left_sing_vals, left_elbow_inds, color=palette[&quot;Left&quot;], ax=ax, label=&quot;Left&quot;) screeplot( right_sing_vals, right_elbow_inds, color=palette[&quot;Right&quot;], ax=ax, label=&quot;Right&quot; ) ax.legend() ax.axvline(n_components, color=&quot;black&quot;, linewidth=1.5, linestyle=&quot;--&quot;) stashfig(&quot;screeplot&quot;) # plot the latent positions before any kind of alignment plot_latents( left_out_latent, right_out_latent, title=&quot;Out latent positions (no alignment)&quot; ) stashfig(&quot;out-latent-no-align&quot;) plot_latents( left_in_latent, right_in_latent, title=&quot;In latent positions (no alignment)&quot; ) stashfig(&quot;in-latent-no-align&quot;) . . /Users/bpedigo/JHU_code/maggot_models/graspologic/graspologic/embed/ase.py:154: UserWarning: Input graph is not fully connected. Results may notbe optimal. You can compute the largest connected component byusing ``graspologic.utils.get_lcc``. /Users/bpedigo/JHU_code/maggot_models/graspologic/graspologic/embed/ase.py:154: UserWarning: Input graph is not fully connected. Results may notbe optimal. You can compute the largest connected component byusing ``graspologic.utils.get_lcc``. Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/screeplot.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/screeplot.png Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-no-align.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-no-align.png Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/in-latent-no-align.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/in-latent-no-align.png . Align the embeddings . Here I align first using Procrustes, and then using Seedless Procrustes initialized at the Procrustes solution. I will refer to this as &quot;oracle initialization&quot;. . &quot;Diff. norm&quot; below refers to $$ | X R - Y |_F$$ where $R$ is learned by one of the Procrustes methods . # collapse def run_alignments(X, Y): op = OrthogonalProcrustes() X_trans_op = op.fit_transform(X, Y) sp = SeedlessProcrustes(init=&quot;custom&quot;, initial_Q=op.Q_) X_trans_sp = sp.fit_transform(X, Y) return X_trans_op, X_trans_sp def calc_diff_norm(X, Y): return np.linalg.norm(X - Y, ord=&quot;fro&quot;) op_left_out_latent, sp_left_out_latent = run_alignments( left_out_latent, right_out_latent ) op_diff_norm = calc_diff_norm(op_left_out_latent, right_out_latent) sp_diff_norm = calc_diff_norm(sp_left_out_latent, right_out_latent) print(f&quot;Procrustes diff. norm using true pairs: {op_diff_norm}&quot;) print(f&quot;Seedless Procrustes diff. norm using true pairs: {sp_diff_norm}&quot;) . . Procrustes diff. norm using true pairs: 10.007906899476886 Seedless Procrustes diff. norm using true pairs: 10.303727955359674 . # collapse plot_latents( op_left_out_latent, right_out_latent, &quot;Out latent positions (Procrustes alignment)&quot; ) stashfig(&quot;out-latent-procrustes&quot;) plot_latents( sp_left_out_latent, right_out_latent, &quot;Out latent positions (Seedless Procrustes alignment, oracle init)&quot;, ) stashfig(&quot;out-latent-seedless-oracle&quot;) . . Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-procrustes.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-procrustes.png Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-seedless-oracle.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/out-latent-seedless-oracle.png . # collapse op_left_in_latent, sp_left_in_latent = run_alignments(left_in_latent, right_in_latent) . . Looking at nearest neighbors in the aligned embeddings . I am concatenating the in and out embeddings after learning the alignments separately for each | &quot;P by K&quot; (short for precision by K) is the proportion of nodes for which their true pair is within its K nearest neighbors | I plot the cumulative density of the measure described above. So for a point x, y on the plot below, the result can be read as &quot;y of all nodes have their true pair within their x nearest neighbors in the aligned embedded space&quot;. | I perform the experiment above using both orthogonal Procrustes (OP) and Seedless Procrustes (SP) with oracale initialization | . # collapse def calc_nn_ranks(target, query): n_pairs = len(target) nn = NearestNeighbors(n_neighbors=n_pairs, metric=&quot;euclidean&quot;) nn.fit(target) neigh_inds = nn.kneighbors(query, return_distance=False) true_neigh_inds = np.arange(n_pairs) _, ranks = np.where((neigh_inds == true_neigh_inds[:, None])) return ranks def plot_rank_cdf(x, ax=None, color=None, label=None, max_rank=51): if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 4)) hist, _ = np.histogram(x, bins=np.arange(0, max_rank, 1)) hist = hist / n_pairs ax.plot(np.arange(1, max_rank, 1), hist.cumsum(), color=color, label=label) ax.set(ylabel=&quot;Cumulative P by K&quot;, xlabel=&quot;K&quot;) return ax op_left_composite_latent = np.concatenate( (op_left_out_latent, op_left_in_latent), axis=1 ) sp_left_composite_latent = np.concatenate( (sp_left_out_latent, sp_left_in_latent), axis=1 ) right_composite_latent = np.concatenate((right_out_latent, right_in_latent), axis=1) op_ranks = calc_nn_ranks(op_left_composite_latent, right_composite_latent) sp_ranks = calc_nn_ranks(sp_left_composite_latent, right_composite_latent) fig, ax = plt.subplots(1, 1, figsize=(8, 4)) plot_rank_cdf(op_ranks, color=palette[&quot;OP&quot;], ax=ax, label=&quot;OP&quot;) plot_rank_cdf(sp_ranks, color=palette[&quot;SP&quot;], ax=ax, label=&quot;SP&quot;) ax.legend() stashfig(&quot;rank-cdf&quot;) . . Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/rank-cdf.pdf Saved figure to maggot_models/notebooks/outs/182.1-BDP-try-align/figs/rank-cdf.png .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/maggot/2020/10/01/maggot-align.html",
            "relUrl": "/pedigo/graspologic/maggot/2020/10/01/maggot-align.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "gzipping a connectome",
            "content": "import gzip edgelist_loc = &quot;maggot_models/data/processed/2020-09-23/G.edgelist&quot; with open(edgelist_loc, &quot;rb&quot;) as f: raw_edgelist = f.read() raw_kb = len(raw_edgelist) / 1000 gzip_edgelist = gzip.compress(raw_edgelist) gzip_kb = len(gzip_edgelist) / 1000 print(f&quot;Raw edgelist size: {raw_kb:.2f} kb&quot;) print(f&quot;Gzipped edgelist size: {gzip_kb:.2f} kb&quot;) print(f&quot;Compression ratio: {gzip_kb/raw_kb:.2f}&quot;) . Raw edgelist size: 2037.68 kb Gzipped edgelist size: 498.20 kb Compression ratio: 0.24 .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/09/25/gzip-connectome.html",
            "relUrl": "/pedigo/graspologic/2020/09/25/gzip-connectome.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Simple random graph models in the latent space framework",
            "content": "import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import seaborn as sns from graspy.embed import AdjacencySpectralEmbed from graspy.models import DCSBMEstimator, EREstimator, SBMEstimator from graspy.models.sbm import _block_to_full, _get_block_indices from graspy.simulations import er_np, sbm rc_dict = { &quot;axes.spines.right&quot;: False, &quot;axes.spines.top&quot;: False, &quot;axes.edgecolor&quot;: &quot;grey&quot;, } for key, val in rc_dict.items(): mpl.rcParams[key] = val context = sns.plotting_context(context=&quot;talk&quot;, font_scale=1) sns.set_context(context) np.random.seed(8888) . Sample from ER model and get expected probabilities . p = 0.3 n = 200 er_graph = er_np(n, p) er_model = EREstimator(directed=False, loops=True) er_model.fit(er_graph) # hacky way of getting ER P matrix er_model.p_mat_[er_model.p_mat_ != 0] = p . Sample from SBM model and get expected probabilities . B = np.array([[0.5, 0.1], [0.1, 0.3]]) community_sizes = [n // 2, n // 2] sbm_graph, labels = sbm(community_sizes, B, return_labels=True, loops=True) # even more hacky way of getting SBM P matrix sbm_model = SBMEstimator(directed=False, loops=True) sbm_model.fit(sbm_graph, y=labels) sbm_model.block_p_ = B _, _, _block_inv = _get_block_indices(labels) sbm_model.p_mat_ = _block_to_full(B, _block_inv, sbm_graph.shape) . Sample from DCSBM model and get expected probabilities . degree_corrections = np.random.beta(2, 2, size=n) for label in np.unique(labels): mask = labels == label degree_corrections[mask] = np.sort(degree_corrections[mask])[::-1] comm_sum = np.sum(degree_corrections[mask]) degree_corrections[mask] = degree_corrections[mask] / comm_sum dcsbm_graph = sbm(community_sizes, B, dc=degree_corrections, loops=True) # super hacky way of getting DCSBM P matrix dcsbm_model = DCSBMEstimator(directed=False, loops=True) dcsbm_model.fit(dcsbm_graph, y=labels) dcsbm_model.block_p_ = B * (n // 2) ** 2 # block_p_ has a different meaning here degree_corrections = degree_corrections.reshape(-1, 1) dcsbm_model.degree_corrections_ = degree_corrections p_mat = _block_to_full(dcsbm_model.block_p_, _block_inv, dcsbm_graph.shape) p_mat = p_mat * np.outer(degree_corrections[:, 0], degree_corrections[:, -1]) dcsbm_model.p_mat_ = p_mat . graphs = [er_graph, sbm_graph, dcsbm_graph] p_mats = [er_model.p_mat_, sbm_model.p_mat_, dcsbm_model.p_mat_] model_names = [&quot;ER&quot;, &quot;SBM&quot;, &quot;DCSBM&quot;] . Embedding the expected and observed graphs with ASE . ase = AdjacencySpectralEmbed(n_components=2, check_lcc=False) # applying a rotation that just makes the plots look nicer theta = np.radians(-30) c, s = np.cos(theta), np.sin(theta) R = np.array(((c, -s), (s, c))) graph_latents = [] for graph in graphs: graph_latent = ase.fit_transform(graph) graph_latents.append(graph_latent @ R) p_mat_latents = [] for p_mat in p_mats: p_mat_latent = ase.fit_transform(p_mat) p_mat_latents.append(p_mat_latent @ R) . Plotting it all together . n_models = len(model_names) n_cols = 4 scale = 3 fig, axs = plt.subplots(n_models, n_cols, figsize=(scale * n_cols, scale * n_models)) def simple_heatmap(mat, ax): sns.heatmap( mat, ax=ax, xticklabels=False, yticklabels=False, cbar=False, cmap=&quot;RdBu_r&quot;, center=0, square=True, vmin=0, vmax=1, ) def simple_scatter(X, ax, y=None): sns.scatterplot( x=X[:, 0], y=X[:, 1], hue=y, s=15, linewidth=0.25, alpha=0.5, ax=ax, legend=False, ) ax.set( xticks=[], yticks=[], xlabel=&quot;&quot;, ylabel=&quot;&quot;, xlim=(-0.4, 1.4), ylim=(-0.4, 1.4) ) y = None for i, model_name in enumerate(model_names): graph = graphs[i] p_mat = p_mats[i] graph_latent = graph_latents[i] p_mat_latent = p_mat_latents[i] if i &gt; 0: y = labels ax = axs[i, 0] simple_heatmap(p_mat, ax) ax.set_ylabel(model_name) ax = axs[i, 1] simple_heatmap(graph, ax) ax = axs[i, 2] simple_scatter(p_mat_latent, ax, y=y) ax = axs[i, 3] simple_scatter(graph_latent, ax, y=y) axs[0, 0].set_title(r&quot;$ mathbf{P} = mathbf{X X^T}$&quot;) axs[0, 1].set_title(r&quot;$ mathbf{G} sim Bernoulli( mathbf{P})$&quot;) axs[0, 2].set_title(r&quot;$ mathbf{X}$&quot;) axs[0, 3].set_title(r&quot;$ mathbf{ hat{X}}$&quot;) . Text(0.5, 1.0, &#39;$ mathbf{ hat{X}}$&#39;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/09/24/latent_model_tutorial.html",
            "relUrl": "/pedigo/graspologic/2020/09/24/latent_model_tutorial.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
  
    
        ,"post10": {
            "title": "Shuffling Alters QAP Results",
            "content": "%pylab inline import sys sys.path sys.path.insert(0, &#39;/Users/asaadeldin/Downloads/GitHub/scipy&#39;) from scipy.optimize import quadratic_assignment . Populating the interactive namespace from numpy and matplotlib . from graspy.simulations import er_corr rho = 0.9 p = 0.5 n = 20 G1, G2 = er_corr(n, p, rho, directed = False, loops = False) . rep = 5 scores = np.zeros(rep) for i in range(rep): res=quadratic_assignment(G1,G2, options={&#39;maximize&#39;:True, &#39;shuffle_input&#39;: False}) scores[i] =res.fun . scores . array([168., 168., 168., 168., 168.]) . rep = 5 scores = np.zeros(rep) for i in range(rep): res=quadratic_assignment(G1,G2, options={&#39;maximize&#39;:True, &#39;rng&#39;: i}) scores[i] =res.fun . scores . array([174., 162., 164., 168., 164.]) .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/31/shuffle_variance.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/31/shuffle_variance.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Can you make the objective function better via gm, compared to the purported 1-1?",
            "content": "# collapse import sys sys.path sys.path.insert(0, &#39;/Users/asaadeldin/Downloads/GitHub/scipy&#39;) from scipy.optimize import quadratic_assignment . . # collapse %pylab inline import pandas as pd from graspy.utils import pass_to_ranks import seaborn as sns . . Populating the interactive namespace from numpy and matplotlib . Experiment Summary . If $A_i$ is the adjacency matrix at time index $i$, then with $n$ time indices, for $i = [1, n-1]$ do $GM(A_i, A_{i+1})$, where $A_i$ and $A_{i+1}$ are pre-matched based on the known 1-1 correspondance. . For each graph pair, run $GM$ $t = 20$ times, with each $t$ corresponding to a different random permutation on $A_{i+1}$. . Internally in GM, $A_{i+1}$ is shuffled, that is $A_{i+1}&#39; = Q A_{i+1} Q^T,$ where $Q$ is sampled uniformly from the set of $m x m$ permutations matrices, where $m$ is the size of the vertex set. . $GM$ is run from the barycenter ($ gamma = 0$). . Compare the objective function values of the matrices with the known matching ($trace (A_i A_{i+1}^T)$) and the average objective function resulting from $t$ runs of $GM(A_i, A_{i+1})$ . # collapse def load_adj(file): df = pd.read_csv(f&#39;org_sig1_max/{file}.csv&#39;, names = [&#39;from&#39;, &#39;to&#39;, &#39;weight&#39;]) return df . . # collapse times = [1,4,11,17,25,34,45,48,52,55,63,69,70,76,80,83,90,97,103,111,117,129,130,132,139,140,146,153,160,167, 174,181,188,192,195,202,209,216,223,229] . . # collapse from scipy.stats import sem t = 20 ofvs = np.zeros((len(times)-1,3)) # [opt_ofv, gm_ofv] for i in range(len(times)-1): # constructing the adjacency matrices Ael = load_adj(times[i]) Bel = load_adj(times[i+1]) nodes = np.concatenate((Ael[&#39;from&#39;],Ael[&#39;to&#39;],Bel[&#39;from&#39;],Bel[&#39;to&#39;]), axis=0) nodes = list(set(nodes)) n = len(nodes) A = np.zeros((n,n)) B = np.zeros((n,n)) row_list_A = [nodes.index(x) for x in Ael[&#39;from&#39;]] col_list_A = [nodes.index(x) for x in Ael[&#39;to&#39;]] A[row_list_A, col_list_A] = Ael[&#39;weight&#39;] row_list_B = [nodes.index(x) for x in Bel[&#39;from&#39;]] col_list_B = [nodes.index(x) for x in Bel[&#39;to&#39;]] B[row_list_B, col_list_B] = Bel[&#39;weight&#39;] A = pass_to_ranks(A) B = pass_to_ranks(B) gm_ofvs = np.zeros(t) for j in range(t): gmp = {&#39;maximize&#39;:True} res = quadratic_assignment(A,B, options=gmp) gm_ofvs[j] = res.fun gm_ofv = np.mean(gm_ofvs) gm_error = sem(gm_ofvs) opt_ofv = (A*B).sum() ofvs[i,:] = [opt_ofv, gm_ofv, 2*gm_error] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.scatter(np.arange(len(times)-1), ofvs[:,0], label = &#39;opt ofv&#39;) #plt.scatter(np.arange(len(times)), ofvs[:,1], label = &#39;gm ofv&#39;) plt.errorbar(np.arange(len(times)-1),ofvs[:,1], ofvs[:,2],label = &#39;average gm ofv +/- 2 s.e.&#39;,marker=&#39;o&#39;, fmt = &#39; &#39; ,capsize=3, elinewidth=1, markeredgewidth=1,color=&#39;orange&#39;) plt.legend() plt.ylabel(&#39;objective function value&#39;) plt.xlabel(&#39;time stamp (A_x &amp; A_{x+1})&#39;) . . Text(0.5, 0, &#39;time stamp (A_x &amp; A_{x+1})&#39;) . Extremely low variance above (error bars not visible) . # collapse plt.scatter(np.arange(len(times)-1), ofvs[:,1]/ofvs[:,0]) plt.hlines(1,0,40,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;y=1 (above means gm maximes ofv more)&#39;) plt.legend() plt.xlabel(&#39;Time Stamp&#39;) plt.ylabel(&#39;gm_ofv / pre-matched_ofv&#39;) . . Text(0, 0.5, &#39;gm_ofv / pre-matched_ofv&#39;) . # collapse df = pd.DataFrame(ofvs,columns=[&quot;Pre Matched OFV&quot;,&quot;Avergae GM OFV&quot;, &quot;2*s.e. GM OFV&quot;]) print(df) . . Pre Matched OFV Avergae GM OFV 2*s.e. GM OFV 0 77.778503 83.203695 6.520387e-15 1 102.363435 105.635371 6.520387e-15 2 539.611816 586.160090 5.216310e-14 3 184.831288 198.412628 2.608155e-14 4 159.324311 203.971815 0.000000e+00 5 984.399156 1152.762630 1.043262e-13 6 1089.477143 1119.627925 0.000000e+00 7 1259.423017 1290.748354 2.086524e-13 8 1585.077755 1730.198821 0.000000e+00 9 1842.297675 2111.213946 2.086524e-13 10 2144.029010 2320.060471 2.086524e-13 11 2031.803286 2092.043201 2.086524e-13 12 1956.629442 2111.907016 2.086524e-13 13 2449.635493 2580.498796 0.000000e+00 14 2488.814694 2536.434492 2.086524e-13 15 2435.361725 2648.848434 2.086524e-13 16 2450.644416 2909.986251 0.000000e+00 17 2799.810641 3086.113044 4.173048e-13 18 2795.063975 3051.412096 2.086524e-13 19 3093.261915 3183.004837 4.173048e-13 20 3483.603820 3653.248466 0.000000e+00 21 3788.502289 3834.393669 2.086524e-13 22 3837.060440 3977.016536 4.173048e-13 23 3625.742003 3800.326356 0.000000e+00 24 3067.810931 3308.850782 2.086524e-13 25 3440.573652 3634.710350 0.000000e+00 26 4605.778803 4669.727638 4.173048e-13 27 4541.053424 4634.822284 4.173048e-13 28 4305.613442 4587.914977 4.173048e-13 29 4103.909802 4283.742180 4.173048e-13 30 3897.553992 4100.491185 4.173048e-13 31 3744.066025 3871.777437 2.086524e-13 32 3206.210282 3391.022125 0.000000e+00 33 3073.053348 3358.433002 4.173048e-13 34 3177.553500 3395.320322 0.000000e+00 35 3332.128518 3368.928739 2.086524e-13 36 3180.781410 3517.218062 0.000000e+00 37 3385.105045 3510.301189 0.000000e+00 38 3229.233901 3407.950538 2.086524e-13 .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/29/known-correspondance.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/29/known-correspondance.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "this is a test",
            "content": "import numpy as np .",
            "url": "https://neurodata.github.io/notebooks/pedigo/test/2020/08/25/test.html",
            "relUrl": "/pedigo/test/2020/08/25/test.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "GM+SS using JAgt Seedless Procrustes",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 75$ and $ vec{n}=[n_1,n_2,n_3] = [25,25,25]$ . for each $ rho in {0.5,0.6, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$, $GM_{SS}$ &amp; $GM_{J.Agt}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ &amp; $GM_{J.Agt}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=50$, $t=10$ . NOTE: The max number of FW iterations here is set at 20. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Description of $GM_{J.Agt}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . With $2^d$ initializations, where $d$ is dimension, use J.Agt&#39;s seedless procrustes to find the optimal orthogonal alignment matrix, $Q$. . let $Phat = hat{X}_1 Q hat{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . # collapse # load in J.Agt data # ratios_j = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) # ratios_ss_j = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) # scores_j = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) # scores_ss_j = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) # rhos = np.arange(5,10.5,0.5) *0.1 # n_p = len(rhos) # ratios_opt_j = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) # ratios_opt_ss_j = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) # scores_opt_j = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) # scores_opt_ss_j = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) . . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) . . # collapse from scipy.stats import sem import seaborn as sns error = np.asarray([sem(ratios_j[i,:]) for i in range(n_p)]) average = np.asarray([np.mean(ratios_j[i,:] ) for i in range(n_p)]) error_j = np.asarray([sem(ratios_ss_j[i,:]) for i in range(n_p)]) average_j = np.asarray([np.mean(ratios_ss_j[i,:] ) for i in range(n_p)]) . . # collapse error_ss = np.asarray([sem(ratios_ss[i,:]) for i in range(n_p)]) average_ss = np.asarray([np.mean(ratios_ss[i,:] ) for i in range(n_p)]) error2 = np.asarray([sem(ratios[i,:]) for i in range(n_p)]) average2 = np.asarray([np.mean(ratios[i,:] ) for i in range(n_p)]) . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) # plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) # plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.errorbar(rhos,average_j, error_j,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+J.Agt&#39;, color=&#39;green&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;r=50, t=10&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7f8588c091c0&gt; . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from .qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr from .jagt import SeedlessProcrustes from graspy.embed import AdjacencySpectralEmbed def run_sim(r, t, n=150, flip=&#39;median&#39;): def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n def _median_sign_flips(X1, X2): X1_medians = np.median(X1, axis=0) X2_medians = np.median(X2, axis=0) val1 = np.sign(X1_medians).astype(int) X1 = np.multiply(val1.reshape(-1, 1).T, X1) val2 = np.sign(X2_medians).astype(int) X2 = np.multiply(val2.reshape(-1, 1).T, X2) return X1, X2 #rhos = 0.1 * np.arange(11)[5:] m = r rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) ratios_opt = np.zeros((n_p,m)) scores_opt = np.zeros((n_p,m)) ratios_opt_ss = np.zeros((n_p,m)) scores_opt_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None ase = AdjacencySpectralEmbed(n_components=3, algorithm=&#39;truncated&#39;) Xhat1 = ase.fit_transform(A1) Xhat2 = ase.fit_transform(A2) if flip==&#39;median&#39;: xhh1, xhh2 = _median_sign_flips(Xhat1, Xhat2) S = xhh1 @ xhh2.T elif flip==&#39;jagt&#39;: sp = SeedlessProcrustes().fit(Xhat1, Xhat2) xhh1 = Xhat1@sp.Q xhh2 = Xhat2 S = xhh1 @ xhh2.T else: S = None for j in range(t): res = quadratic_assignment_sim(A1, A2, True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, S, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1, A2, True, S, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_opt_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_opt_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.legend() plt.savefig(&#39;figure_matchratio.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/24/jagt-gm.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/24/jagt-gm.html",
            "date": " • Aug 24, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Recreating Youngser's R code figures",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . All values were set to best replicate Youngser&#39;s Figure . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0.5,0.6, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ s.e. . This notebook contains figures for $r=50$, $t=10$ . NOTE: The max number of FW iterations here is set at 20 to best replicate Youngser R results. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 odds = [i for i in range(len(rhos)) if i%2==0] n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = np.asarray([sem(ratios[i,:]) for i in range(n_p)]) average = np.asarray([np.mean(ratios[i,:] ) for i in range(n_p)]) error_ss = np.asarray([sem(ratios_ss[i,:]) for i in range(n_p)]) average_ss = np.asarray([np.mean(ratios_ss[i,:] ) for i in range(n_p)]) . . Ali&#39;s Figure (with Python) . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;r=50, t=10&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f8c33d30&gt; . Youngser&#39;s Figure (with R) . . Eyeballing both onto the same figure . # collapse gm_ss_y = [0.01, 0.03, 0.12, 0.42, 0.77, 1.0] gm_y = [0, 0, 0.03, 0.15, 0.62, 1.0] sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos[odds],average_ss[odds], error_ss[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;, color=&#39;blue&#39;) plt.plot(rhos[odds],gm_ss_y, color=&#39;blue&#39;, marker=&#39;^&#39;) plt.errorbar(rhos[odds],average[odds], error[odds],marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.plot(rhos[odds],gm_y, color=&#39;red&#39;, marker=&#39;^&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5, &#39;triangles = youngser, circle = ali&#39;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5ef18bbe0&gt; . # collapse ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . Call &#39;best case&#39; the instance where $Q$ sampled uniformly from the set of $n x n$ permutations matrices is equal to the identity matrix. . As we see from the figures below, for vanilla GM, the best case seems to consistently perform better, but with GM+SS they appear to be consistently about the same . GM+SS . # collapse diff = scores_opt_ss[9,:] - scores_ss[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt_ss[9,:] - ratios_ss[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . GM . # collapse diff = scores_opt[9,:] - scores[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt[9,:] - ratios[9,:] plt.hist(diff, bins=10) plt.vlines(0,0,50,linestyles=&#39;dashed&#39;, color = &#39;red&#39;, label=&#39;x=0&#39;) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from .qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def run_sim(r, t): def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = r #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) ratios_opt = np.zeros((n_p,m)) scores_opt = np.zeros((n_p,m)) ratios_opt_ss = np.zeros((n_p,m)) scores_opt_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores_opt, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_opt_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_opt_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) txt =f&#39;r={r}, t={t}&#39; plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.text(0.5,0.5,txt) plt.legend() plt.savefig(&#39;figure_matchratio.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/19/recreate-youngser.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/19/recreate-youngser.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Graph matching with spectral similarity (8-18, r=100, t=30, 'max_iter' = 20)",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=100$, $t=30$ . NOTE: The max number of FW iterations here is set at 20 to best replicate Youngser R results. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(12,8)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f6a5fd90&gt; . #collapse ratios_opt = np.genfromtxt(&#39;ratios_opt.csv&#39;, delimiter = &#39;,&#39;) ratios_opt_ss = np.genfromtxt(&#39;ratios_opt_ss.csv&#39;, delimiter=&#39;,&#39;) scores_opt = np.genfromtxt(&#39;scores_opt.csv&#39;, delimiter = &#39;,&#39;) scores_opt_ss = np.genfromtxt(&#39;scores_opt_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . Call &#39;best case&#39; the instance where $Q$ sampled uniformly from the set of $n x n$ permutations matrices is equal to the identity matrix . GM+SS . # collapse diff = scores_opt_ss[9,:] - scores_ss[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . # collapse diff = ratios_opt_ss[9,:] - ratios_ss[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . GM . #collapse diff = scores_opt[9,:] - scores[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Objective Value Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . #collapse diff = ratios_opt[9,:] - ratios[9,:] plt.hist(diff, bins=10) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (&quot;Best Case&quot; - argmax_t[objective_t])&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 100 t = 30 #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;r_100_t_50.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(2).html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(2).html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Graph matching with spectral similarity (8-18, r=100, t=30, 'max_iter' = 30)",
            "content": "# collapse import matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from graspy.simulations import sbm_corr . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter ($ gamma = 0$). . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=100$, $t=30$ . NOTE: The max number of FW iterations here is set at 30. . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . Code included at the bottom, which was run on a remote server . # collapse ratios = np.genfromtxt(&#39;ratios.csv&#39;, delimiter = &#39;,&#39;) ratios_ss = np.genfromtxt(&#39;ratios_ss.csv&#39;, delimiter=&#39;,&#39;) scores = np.genfromtxt(&#39;scores.csv&#39;, delimiter = &#39;,&#39;) scores_ss = np.genfromtxt(&#39;scores_ss.csv&#39;, delimiter=&#39;,&#39;) rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) . . # collapse from scipy.stats import sem import seaborn as sns error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] . . # collapse sns.set_context(&#39;paper&#39;) sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() . . &lt;matplotlib.legend.Legend at 0x7fa5f5ed1430&gt; . Script to run simulations . # collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed from qap_sim import quadratic_assignment_sim import seaborn as sns from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 100 t = 30 #rhos = 0.1 * np.arange(11)[5:] rhos = np.arange(5,10.5,0.5) *0.1 n_p = len(rhos) ratios = np.zeros((n_p,m)) scores = np.zeros((n_p,m)) ratios_ss = np.zeros((n_p,m)) scores_ss = np.zeros((n_p,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False for k, rho in enumerate(rhos): np.random.seed(8888) seeds = [np.random.randint(1e8, size=t) for i in range(m)] def run_sim(seed): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed[j]}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratio = match_ratio(res_opt[&#39;col_ind&#39;], n) score = res_opt[&#39;score&#39;] ratio_ss = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) score_ss = res_opt_ss[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt = match_ratio(res[&#39;col_ind&#39;], n) score_opt = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;shuffle_input&#39;:False}) ratio_opt_ss = match_ratio(res[&#39;col_ind&#39;], n) score_opt_ss = res[&#39;score&#39;] return ratio, score, ratio_ss, score_ss, ratio_opt, score_opt, ratio_opt_ss, score_opt_ss result = Parallel(n_jobs=-1, verbose=10)(delayed(run_sim)(seed) for seed in seeds) ratios[k,:] = [item[0] for item in result] scores[k,:] = [item[1] for item in result] ratios_ss[k,:] = [item[2] for item in result] scores_ss[k,:] = [item[3] for item in result] ratios_opt[k,:] = [item[4] for item in result] scores_opt[k,:] = [item[5] for item in result] ratios_opt_ss[k,:] = [item[6] for item in result] scores_opt_ss[k,:] = [item[7] for item in result] np.savetxt(&#39;ratios.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt.csv&#39;,ratios, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt.csv&#39;,scores, delimiter=&#39;,&#39;) np.savetxt(&#39;ratios_opt_ss.csv&#39;,ratios_ss, delimiter=&#39;,&#39;) np.savetxt(&#39;scores_opt_ss.csv&#39;,scores_ss, delimiter=&#39;,&#39;) from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(n_p)] average = [np.mean(ratios[i,:] ) for i in range(n_p)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(n_p)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(n_p)] sns.set_context(&#39;paper&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;r_100_t_50.png&#39;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(1).html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/18/ali-gm-ss(1).html",
            "date": " • Aug 18, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Graph matching with spectral similarity",
            "content": "#collapse from graspy.match import GraphMatch as GMP from graspy.simulations import sbm_corr from graspy.embed import AdjacencySpectralEmbed . . #collapse import numpy as np import matplotlib.pyplot as plt import random import sys from joblib import Parallel, delayed import seaborn as sns . . #collapse from qap_sim import quadratic_assignment_sim . . Experiment Summary . Let $(G_1, G_2) sim rho-SBM( vec{n},B)$. (NB: binary, symmetric, hollow.) . $K = 3$. . the marginal SBM is conditional on block sizes $ vec{n}=[n_1,n_2,n_3]$. . $B = [(.20,.01,.01);(.01,.10,.01);(.01,.01,.20)]$. (NB: rank($B$)=3 with evalues $ approx [0.212,0.190,0.098]$.) . with $n = 150$ and $ vec{n}=[n_1,n_2,n_3] = [50,50,50]$ . for each $ rho in {0,0.1, cdots,0.9,1.0 }$ generate $r$ replicates $(G_1, G_2)$. . For all $r$ replicates, run $GM$ and $GM_{SS}$ each $t$ times, with each $t$ corresponding to a different random permutation on $G_2$. . Specifically,$G_2&#39; = Q G_2 Q^T,$ where $Q$ is sampled uniformly from the set of $n x n$ permutations matrices. . For each $t$ permutation, run $GM$ &amp; $GM_{SS}$ from the barycenter. . For each $r$, the $t$ permutation with the highest associated objective function value will have it&#39;s match ratio recorded . For any $ rho$ value, have $ delta$ denote the average match ratio over the $r$ realizations . Plot $x= rho$ vs $y$= $ delta$ $ pm$ 2s.e. . This notebook contains figures for $r=50$, $t=20$ . Description of $GM_{ss}$ Procedure . For each $r$, ASE each graph into $d=3$ yielding $ hat{X}_1$ &amp; $ hat{X}_2$ . MedianFlip both into the first orthant yielding $ bar{X}_1$ &amp; $ bar{X_2}$ . let $Phat = bar{X}_1 bar{X}_2^T$ and run $t$ repititions of gm with $G_1,G_2 and Phat$ as the similarity. . #collapse def match_ratio(inds, n): return np.count_nonzero(inds == np.arange(n)) / n n = 150 m = 1 t = 10 rhos = 0.1 * np.arange(11) ratios2 = np.zeros((11,m)) scores2 = np.zeros((11,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False . . #collapse n = 150 m = 50 t = 20 rhos = 0.1 * np.arange(11) ratios = np.zeros((11,m)) scores = np.zeros((11,m)) ratios_ss = np.zeros((11,m)) scores_ss = np.zeros((11,m)) n_per_block = int(n/3) n_blocks = 3 block_members = np.array(n_blocks * [n_per_block]) block_probs = np.array([[0.2, 0.01, 0.01], [0.01, 0.1, 0.01], [0.01, 0.01, 0.2]]) directed = False loops = False #np.random.seed(8888) for k, rho in enumerate(rhos): for i in range(m): A1, A2 = sbm_corr( block_members, block_probs, rho, directed=directed, loops=loops ) score = 0 res_opt = None score_ss = 0 res_opt_ss = None for j in range(t): seed = k+m+t res = quadratic_assignment_sim(A1,A2, sim=False, maximize=True, options={&#39;seed&#39;:seed}) if res[&#39;score&#39;]&gt;score: res_opt = res score = res[&#39;score&#39;] res = quadratic_assignment_sim(A1,A2, sim=True, maximize=True, options={&#39;seed&#39;:seed}) if res[&#39;score&#39;]&gt;score_ss: res_opt_ss = res score_ss = res[&#39;score&#39;] ratios[k,i] = match_ratio(res_opt[&#39;col_ind&#39;], n) scores[k,i] = res_opt[&#39;score&#39;] ratios_ss[k,i] = match_ratio(res_opt_ss[&#39;col_ind&#39;], n) scores_ss[k,i] = res_opt_ss[&#39;score&#39;] #ratios[k] = ratios[k]/m . . #collapse from scipy.stats import sem error = [2*sem(ratios[i,:]) for i in range(11)] average = [np.mean(ratios[i,:] ) for i in range(11)] error_ss = [2*sem(ratios_ss[i,:]) for i in range(11)] average_ss = [np.mean(ratios_ss[i,:] ) for i in range(11)] . . #collapse sns.set_context(&#39;talk&#39;) #sns.set(rc={&#39;figure.figsize&#39;:(15,10)}) plt.errorbar(rhos,average_ss, error_ss,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM+SS&#39;) plt.errorbar(rhos,average, error,marker=&#39;o&#39;,capsize=3, elinewidth=1, markeredgewidth=1, label=&#39;GM&#39;, color=&#39;red&#39;) plt.xlabel(&quot;rho&quot;) plt.ylabel(&quot;avergae match ratio&quot;) plt.legend() plt.savefig(&#39;GM_GM+SS.png&#39;,fmt=&quot;png&quot;, dpi=150, facecolor=&quot;w&quot;, bbox_inches=&quot;tight&quot;, pad_inches=0.3) . . &lt;ipython-input-111-9d9e37bc5d45&gt;:8: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument &#34;fmt&#34; which is no longer supported as of 3.3 and will become an error two minor releases later plt.savefig(&#39;GM_GM+SS.png&#39;,fmt=&#34;png&#34;, dpi=150, facecolor=&#34;w&#34;, bbox_inches=&#34;tight&#34;, pad_inches=0.3) . #collapse diff = ratios_ss[9,:] - ratios[9,:] plt.hist(diff, bins=20) plt.ylabel(&#39;Density&#39;) plt.xlabel(&#39;Match Ratio Difference (GM+SS - GM)&#39;) plt.title(&#39;Paired Difference Histogram (Rho = 0.9)&#39;) . . Text(0.5, 1.0, &#39;Paired Difference Histogram (Rho = 0.9)&#39;) . #collapse left_adj = np.genfromtxt(&#39;left_adj.csv&#39;, delimiter=&#39;,&#39;) right_adj = np.genfromtxt(&#39;right_adj.csv&#39;, delimiter=&#39;,&#39;) . . #collapse def median_sign_flips(X1, X2): X1_medians = np.median(X1, axis=0) X2_medians = np.median(X2, axis=0) val = np.multiply(X1_medians, X2_medians) t = (val &gt; 0) * 2 - 1 X1 = np.multiply(t.reshape(-1, 1).T, X1) return X1, X2 . .",
            "url": "https://neurodata.github.io/notebooks/graph-matching/ali-s-e/2020/08/16/ali-gm-ss.html",
            "relUrl": "/graph-matching/ali-s-e/2020/08/16/ali-gm-ss.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Comparing multiple graph samples over time using latent distributions",
            "content": "import time import matplotlib as mpl import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from graspy.inference import LatentDistributionTest from graspy.simulations import p_from_latent, sample_edges from graspy.utils import symmetrize from hyppo.discrim import DiscrimOneSample from sklearn.metrics import pairwise_distances np.random.seed(8888) sns.set_context(&quot;talk&quot;) mpl.rcParams[&quot;axes.edgecolor&quot;] = &quot;lightgrey&quot; mpl.rcParams[&quot;axes.spines.right&quot;] = False mpl.rcParams[&quot;axes.spines.top&quot;] = False def hardy_weinberg(theta): &quot;&quot;&quot; Maps a value from [0, 1] to the hardy weinberg curve. &quot;&quot;&quot; hw = [theta ** 2, 2 * theta * (1 - theta), (1 - theta) ** 2] return np.array(hw).T def sample_hw_graph(thetas): latent = hardy_weinberg(thetas) p_mat = p_from_latent(latent, rescale=False, loops=False) graph = sample_edges(p_mat, directed=False, loops=False) return (graph, p_mat, latent) . Parameters of the experiment . n_timepoints = 5 n_verts = 100 n_graphs_per_timepoint = 10 deltas = np.linspace(0, 2, n_timepoints) . Distributions in latent space . Let $HW( theta)$ be the Hardy-Weinberg distribution in $ mathbb{R}^3$. . Latent positions are distributed along this curve: $$X sim HW( theta)$$ With the distribution along the curve following a Beta distribution: $$ theta sim Beta(1, 1 + delta)$$ Let $ delta$ be a proxy for &quot;time&quot; . Below I plot the distributions of $ theta$ for each value of $ delta$, where we will use a different value of $ delta$ for each time point. . fig, ax = plt.subplots(1, 1, figsize=(8, 4)) for delta in deltas: thetas = np.random.beta( 1, 1 + delta, 10000 ) # fake # to make the distributions look cleaner sns.distplot(thetas, label=delta, ax=ax) plt.legend(title=r&quot;$ delta$&quot;, bbox_to_anchor=(1, 1), loc=&quot;upper left&quot;) _ = ax.set(ylabel=&quot;Frequency&quot;, yticks=[], xlabel=r&quot;$ theta$&quot;) . Sample latent positions, and then sample graphs . To generate each graph I sample a set of latent positions from the Hardy-Weinberg curve described above. Each time point will have multiple sets of latent positions sampled i.i.d. from the same distribution in latent space, then a single graph is sampled from each set of latent positions. . graphs = [] latents = [] times = [] for t, delta in enumerate(deltas): for i in range(n_graphs_per_timepoint): thetas = np.random.beta(1, 1 + delta, n_verts) graph, pmat, latent = sample_hw_graph(thetas) graphs.append(graph) times.append(t) latents.append(latent) times = np.array(times) . Plot 2 example sets of sampled latent positions for each time point . Here I just show the first two dimensions of true latent positions. From each of these we sample a graph. . fig, axs = plt.subplots( 2, n_timepoints, figsize=(n_timepoints * 4, 8), sharex=True, sharey=False, # TODO fix sharey and labeling ) for t, delta in enumerate(deltas): for i in range(2): ax = axs[i, t] latent = latents[t * n_graphs_per_timepoint + i] plot_latent = pd.DataFrame(latent) sns.scatterplot(data=plot_latent, x=0, y=1, ax=ax, linewidth=0, alpha=0.5, s=20) ax.set(xlabel=&quot;&quot;, ylabel=&quot;&quot;, xticks=[], yticks=[]) if i == 0: deltastr = r&quot;$ delta$&quot; + f&quot; = {deltas[t]}&quot; ax.set_title(f&quot;t = {t} ({deltastr})&quot;) if t == 0: ax.set_ylabel(f&quot;Sample {i + 1}&quot;) plt.tight_layout() . Plot adjacency matrices for 2 graphs from each time point . fig, axs = plt.subplots(2, n_timepoints, figsize=(n_timepoints * 4, 8)) for t, delta in enumerate(deltas): for i in range(2): graph = graphs[t * n_graphs_per_timepoint + i] ax = axs[i, t] sns.heatmap( graph, ax=ax, cbar=False, xticklabels=False, yticklabels=False, cmap=&quot;RdBu_r&quot;, square=True, center=0, ) if i == 0: deltastr = r&quot;$ delta$&quot; + f&quot; = {deltas[t]}&quot; ax.set_title(f&quot;t = {t} ({deltastr})&quot;) if t == 0: ax.set_ylabel(f&quot;Sample {i + 1}&quot;) plt.tight_layout() . Compute the test statistics for Latent Distribution Test (nonpar). . curr_time = time.time() pval_mat = np.zeros((len(graphs), len(graphs))) tstat_mat = np.zeros((len(graphs), len(graphs))) n_comparisons = (len(graphs) * (len(graphs) - 1)) / 2 counter = 0 for i, graph1 in enumerate(graphs): for j, graph2 in enumerate(graphs): if i &lt; j: ldt = LatentDistributionTest(n_bootstraps=200, workers=1) ldt.fit(graph1, graph2) pval_mat[i, j] = ldt.p_value_ tstat_mat[i, j] = ldt.sample_T_statistic_ pval_mat = symmetrize( pval_mat, method=&quot;triu&quot; ) # need to do way more bootstraps to be meaningful tstat_mat = symmetrize(tstat_mat, method=&quot;triu&quot;) print(f&quot;{(time.time() - curr_time)/60:.3f} minutes elapsed&quot;) . 5.981 minutes elapsed . All pairwise test statistics and p-values . Here I show test statistics for the latent position test between all possible pairs of graphs. Higher means more different. The test statistic being used here is the 2-sample dcorr test statistic on the estimated latent positions. Note that I&#39;m not doing the new seedless alignment here (but I&#39;d like to). . Then, I show the same for the p-values. . fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.heatmap( tstat_mat, ax=ax, xticklabels=False, yticklabels=False, cmap=&quot;Reds&quot;, square=True, cbar_kws=dict(shrink=0.7), ) line_kws = dict(linestyle=&quot;-&quot;, linewidth=1, color=&quot;grey&quot;) for t in range(1, n_timepoints): ax.axvline(t * n_graphs_per_timepoint, **line_kws) ax.axhline(t * n_graphs_per_timepoint, **line_kws) tick_locs = ( np.arange(0, n_timepoints * n_graphs_per_timepoint, n_graphs_per_timepoint) + n_graphs_per_timepoint / 2 ) ax.set( xticks=tick_locs, xticklabels=np.arange(n_timepoints), xlabel=&quot;Time point&quot;, title=&quot;Latent distribution test statistics&quot;, ) fig, ax = plt.subplots(1, 1, figsize=(8, 8)) sns.heatmap( pval_mat, ax=ax, xticklabels=False, yticklabels=False, cmap=&quot;Reds&quot;, square=True, cbar_kws=dict(shrink=0.7), ) line_kws = dict(linestyle=&quot;-&quot;, linewidth=1, color=&quot;grey&quot;) for t in range(1, n_timepoints): ax.axvline(t * n_graphs_per_timepoint, **line_kws) ax.axhline(t * n_graphs_per_timepoint, **line_kws) tick_locs = ( np.arange(0, n_timepoints * n_graphs_per_timepoint, n_graphs_per_timepoint) + n_graphs_per_timepoint / 2 ) _ = ax.set( xticks=tick_locs, xticklabels=np.arange(n_timepoints), xlabel=&quot;Time point&quot;, title=&quot;Latent distribution test p-values&quot;, ) . Computing discriminability . Looks at whether distances between samples from the same object (time point, in this case) are smaller than distances between samples from different objects. In a sense, it&#39;s looking at whether the diagonal blocks in the above are smaller than the rest of the matrix. Here I&#39;m using the test statistic from above as the distance. Permutation test is used to test whether one&#39;s ability to discriminate between &quot;multiple samples&quot; from the same object is highter than one would expect by chance. . curr_time = time.time() discrim = DiscrimOneSample(is_dist=True) discrim.test(tstat_mat, times) print(f&quot;Discriminability one-sample p-value: {discrim.pvalue_}&quot;) print(f&quot;Discriminability test statistic: {discrim.stat}&quot;) print(f&quot;{(time.time() - curr_time)/60:.3f} minutes elapsed&quot;) . Discriminability one-sample p-value: 0.001 Discriminability test statistic: 0.8648333333333332 0.061 minutes elapsed . Test statistics and p-values as a function of time difference . Here I just play with plotting these test statistics and p-values as a function of how different in time the two graphs were. I add jitter to the time difference values just for visibility. . time_dist_mat = pairwise_distances(times.reshape((-1, 1)), metric=&quot;manhattan&quot;) triu_inds = np.triu_indices_from(time_dist_mat, k=1) time_dists = time_dist_mat[triu_inds] + np.random.uniform(-0.2, 0.2, len(triu_inds[0])) latent_dists = tstat_mat[triu_inds] fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.scatterplot(x=time_dists, y=latent_dists, s=10, linewidth=0, alpha=0.3, ax=ax) ax.set(ylabel=&quot;Test statistic&quot;, xlabel=&quot;Difference in time&quot;) pval_dists = pval_mat[triu_inds] fig, ax = plt.subplots(1, 1, figsize=(8, 4)) sns.scatterplot(x=time_dists, y=pval_dists, s=10, linewidth=0, alpha=0.3, ax=ax) _ = ax.set(ylabel=&quot;p-value&quot;, xlabel=&quot;Difference in time&quot;) .",
            "url": "https://neurodata.github.io/notebooks/pedigo/graspologic/2020/07/23/multi-time-latents.html",
            "relUrl": "/pedigo/graspologic/2020/07/23/multi-time-latents.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://neurodata.github.io/notebooks/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://neurodata.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://neurodata.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://neurodata.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}